\documentclass[11pt]{article}

\usepackage{booktabs}

\setlength\topmargin{-0.6cm}   
\setlength\textheight{23.4cm}
\setlength\textwidth{17.0cm}
\setlength\oddsidemargin{0cm} 
 

\begin{document}

\begin{center}
\LARGE
LING572 HW9: Neural Networks\\
Due: 11pm on March 19, 2020\\
\vspace{0.3in}
\end{center}

This assignment explores text classification using a version of the Deep Averaging Network from Iyyer et al 2015, as discussed in the first half of the Recurrent Neural Network lectures on applying MLPs to text classification.  Through the assignment, you will:
\begin{itemize}
	\item Get familiar with the basics of doing text classification in PyTorch.  We will ``pseudo-reproduce'' a result from Iyyer et al by training a Deep Averaging Network on the IMDb Reviews dataset for sentiment classification.  Its a pseudo-reproduction for a few reasons:
		\begin{itemize}
			\item We are using a slightly different dataset split (17.5k train instead of 25k)
			\item Our model will have 2 hidden layers instead of 3, just for compute efficiency
		\end{itemize}
	\item Implement a linear layer, the basic building block of neural networks.
	\item Implement L2 regularization, and see how it impacts performance and runtime.
	\item Implement early stopping.
\end{itemize}
We have provided the bulk of the necessary code.  You will have to fill in some blank spots for each of the implementation questions, described in more detail below.

In the directory \texttt{/dropbox/19-20/572/hw9/} you will find the following:
\begin{itemize}
	\item env: the environment; no need to touch this
	\item data: the IMDB dataset
	\item model.py: defines a linear layer and a Deep Averaging Network
	\item main.py: the main script for building and training a model

		NB: at the bottom of the file, you will see many command-line arguments.  \texttt{python main.py} will run with the defaults, but you can also set these.  For example, to train for 10 epochs, you can use \texttt{python main.py --num\_epochs 10}.
	\item run\_hw9.sh: an example executable to modify / submit via condor.  Note: you will want to change the `\texttt{cd \ldots}' line to point to your directory. Note that this file shows you how to activate the supplied conda environment.
\end{itemize}


%%%%%%%%%%%%%%%%%%%
\vspace{0.4 in}
\noindent
{\bf Q0 (10 points):} Install Anaconda. This is a necessary component for running the code for this assignment.  I have setup a conda environment for the assignment, but you will need to install anaconda in order to use that environment.  These are ``free points''.  From your home directory, please execute the following steps:
\begin{enumerate}
	\item \texttt{wget https://repo.anaconda.com/archive/Anaconda3-2019.10-Linux-x86\_64.sh}
	\item \texttt{sh Anaconda3-2019.10-Linux-x86\_64.sh}
\end{enumerate}
The first two lines of \texttt{run\_hw9.sh} show how to now activate the environment that we have supplied with all of the necessary libraries.


%%%%%%%%%%%%%%%%%%%
\vspace{0.4 in}
\noindent
{\bf Q1 (20 points):} Implement the forward pass of a LinearLayer and train a model.  
\begin{itemize}
	\item Find the `\# TODO:' comment in \texttt{/dropbox/19-20/572/hw9/model.py}.  Implement the .forward method there, following the instructions in the comment and the docstring.
	\item Run \texttt{python main.py --num\_epochs 6 > q1.out}.  Please fill out the information below:
		\begin{center}
		\begin{tabular}[h]{ccc}
			\toprule
			Epoch num & Train loss & Dev loss \\
			\midrule
			0 & & \\
			1 & & \\
			2 & & \\
			3 & & \\
			4 & & \\
			5 & & \\
			\bottomrule
		\end{tabular}
		\end{center}
		Test set accuracy of best model:

		Total runtime:
\end{itemize}


%%%%%%%%%%%%%%%%%%%
\vspace{0.4 in}
\noindent
{\bf Q2 (20 points):} $L_2$ regularization prevents over-fitting by penalizing large weight values.  In particular, if $\mathcal{L}(\theta)$ is our loss function, $L_2$ regularization replaces that loss with
\[ \mathcal{L}'(\theta) = \mathcal{L}(\theta) + \lambda \| \theta \|_2^2 \]
where $\|\theta\|_2^2$ is the squared $L^2$ norm (i.e. the sum of the squares of all parameters in $\theta$). You will:
\begin{itemize}
	\item Add $L_2$ regularization in \texttt{main.py}.  Search for `\# TODO:' and find the one right above the line \texttt{L2 = 0.0}.  Replace this with the squared $L_2$ norm of the model's parameters.

		Hint: \texttt{model.parameters()} returns an iterator over the parameters, which are each a \texttt{torch.tensor}.

		Note: you should use the \texttt{--L2} flag (stored in \texttt{args.L2}) to only compute the regularization term when requested from the command-line.
	\item Run \texttt{python main.py --num\_epochs 6 --L2 > q2.out}, training for 6 epochs with $L_2$ regularization.  Please fill out the information below:
		\begin{center}
		\begin{tabular}[h]{ccc}
			\toprule
			Epoch num & Train loss & Dev loss \\
			\midrule
			0 & & \\
			1 & & \\
			2 & & \\
			3 & & \\
			4 & & \\
			5 & & \\
			\bottomrule
		\end{tabular}
		\end{center}
		Test set accuracy of best model:

		Total runtime:
\end{itemize}


%%%%%%%%%%%%%%%%%%%
\vspace{0.4 in}
\noindent
{\bf Q3 (20 points):} Another method for preventing over-fitting is early stopping.  On this approach, we define a hyper-parameter patience ($p$), which is an integer.  We then train for a large number of epochs, but if loss on the dev set is worse at epoch $t$ than at epoch $t-p$ for any epoch, we stop training immediately.
\begin{itemize}
	\item Implement early stopping.  The final `\# TODO:' in \texttt{main.py} occurs instructs you to implement this early stopping protocol in the main training loop.  

		Note: you may need to edit code outside the immediate `if' statement that the comment appears in.
	\item Run \texttt{python main.py --num\_epochs 12 --patience 3 --L2 > q3.out} and fill out the information below (if early stopping stops your model before 12 epochs, you will have empty rows in this table, which is acceptable):
		\begin{center}
		\begin{tabular}[h]{ccc}
			\toprule
			Epoch num & Train loss & Dev loss \\
			\midrule
			0 & & \\
			1 & & \\
			2 & & \\
			3 & & \\
			4 & & \\
			5 & & \\
			6 & & \\
			7 & & \\
			8 & & \\
			9 & & \\
			10 & & \\
			11  & & \\
			\bottomrule
		\end{tabular}
		\end{center}
		Test set accuracy of best model:

		Total runtime:
\end{itemize}



%%%%%%%%%%%%%%%%%%%
\vspace{0.4 in}
\noindent
{\bf Q4 (5 points):} We will issue 5 total points based on the runtimes reported in Q1-3.  They should be approximately no more than 10 minutes when running on patas.  No need to submit anything for this question.


%%%%%%%%%%%%%%%%%%%
\vspace{0.5 in}
\noindent
{\bf Submission:}  Submit the following to Canvas:

\begin{itemize}
    \item Your note file {\it readme.(txt $\mid$ pdf)}
    that includes the tables and additional information above, and any notes that you want the TA to read.

  \item  hw.tar.gz that includes all the files specified in
      dropbox/19-20/572/hw9/submit-file-list, plus any source code
      (and binary code) used by the shell scripts.

  \item Make sure that you run {\bf check\_hw9.sh} before
    submitting your hw.tar.gz.
\end{itemize}

\end{document}
