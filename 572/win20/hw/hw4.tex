\documentclass[11pt]{article}

\setlength\topmargin{-0.6cm}   
\setlength\textheight{23.4cm}
\setlength\textwidth{17.0cm}
\setlength\oddsidemargin{0cm} 
 

\begin{document}

\begin{center}
\LARGE
LING572 Hw4 (kNN)\\
Due: 11pm on Feb 6, 2020\\



\vspace{0.3in}
\end{center}


The example files are under dropbox/19-20/572/hw4/examples/.

% Remember from now on, when you print out a float number in the system output file, round the number to {\bf five decimal places} (e.g., 3.1415926 becomes 3.14159).


\vspace{0.3 in}
\noindent {\bf Q1 (40 points):} Write a script, {\bf build\_kNN.sh}, 
that implements the kNN algorithm. It classifies a test instance x 
by letting the k nearest neighbors of x vote. 
\begin{itemize} 
  \item The learner should treat features as real-valued.

  \item Use majority vote; that is, each of the k nearest neighbors
         has one vote.

  \item The format is: build\_kNN.sh training\_data test\_data k\_val similarity\_func sys\_output $>$ acc\_file 

  \item training\_data and test\_data are the vector files in the text format
        (cf. {\bf train.vectors.txt}).

  \item k\_val is the value of {\it k}; i.e., 
        the number of nearest neighbors chosen for classification.
  
  \item similarity\_func is the id of the similarity function. 
        If the variable is 1, use Euclidean distance. If the value is 2,
          use Cosine function. {\bf Notice that Euclidean distance is a 
          dissimilarity measure; that is, the longer the distance 
          between two instances is,
          the more dissimilar (i.e., the less similar) the instances are.}

  \item sys\_output and acc\_file have the same format as the one 
    specified in Hw3, and they should include the classification results
    for both training and test data. When choosing k nearest neighbors
    for a training instance $x$, one of those neighbors is $x$ itself.
    Notice that since the other k-1 neighbors could have labels different from
    that of $x$, the training accuracy could be lower than 100\%.

  \item For each line of sys\_output, remember to sort
      the $(c_i, p_i)$ pairs by the value
      of $p_i$ in {\bf descending order}.

  \item Please submit output files correspond to $k=5, similarity=cosine$.
\end{itemize}


Run build\_kNN.sh with {\bf train.vectors.txt} as the 
training data and {\bf test.vectors.txt} as the test data.
Fill out Table 1 with different values of k and similarity function.


\begin{table}[h]
\centering
\caption{Test accuracy using {\bf real-valued} features} 
\label{table1}
\begin{tabular}{|r|l|l|} \hline
k  & Euclidean distance & Cosine function \\ \hline
1       &  &    \\ \hline
5       &  &    \\ \hline
10      &  &    \\ \hline
\end{tabular}
\end{table}


%%%%%%%%%%%%%%%%%%%%%%%
\vspace{0.4in}
\noindent
{\bf Q2 (35 points):} Write a script, rank\_feat\_by\_chi\_square.sh, that ranks
          features by $\chi^2$ scores. 
\begin{itemize}
   \item The format for the command line is: cat input\_file $\mid$ rank\_feat\_by\_chi\_square.sh $>$ output\_file
   
   \item input\_file is a feature vector file in the text format
         (e.g., {\bf train.vectors.txt}).

   \item The output\_file has the format ``featName score docFreq''. 
         The score is the chi-square score for the feature; docFreq
         is the number of documents that the feature occurs in.
         The lines are sorted by  $\chi^2$\ scores in descending order. 

   \item For $\chi^2$ calculation, treat each feature as binary; 
          that is, suppose the input\_file has $a_i$ instances
          with class label $c_i$. Out of these $a_i$ 
          instances, $b_i$ of them contain the feature $f_k$, 
          then the corresponding contingency table for 
          feature $f_k$ is shown in Table \ref{table-Q1}.

   \item Run ``cat train.vectors.txt $\mid$ rank\_feat\_by\_chi\_square.sh $>$ feat\_list'' and submit feat\_list.

\end{itemize}

\begin{table}[bph]
\centering
\caption{A contingency table for feature $f_k$}
\label{table2}
\begin{tabular}{|l|l|l|l|} \hline
     &  $c_1$ & $c_2$  & $c_3$ \\ \hline
$\bar{f_k}$ & $a_1 - b_1$ & $a_2 - b_2$  & $a_3 - b_3$ \\ \hline
$f_k$ & $b_1$ & $b_2$  & $b_3$ \\ \hline

\end{tabular}
\label{table-Q1}
\end{table}




%%%%%%%%%%%%%%%%%%%%%%%%

\vspace{0.5 in}
\noindent
{\bf Submission:}  Submit the following to Canvas:

\begin{itemize}
  \item Your note file {\it readme.(txt $\mid$ pdf)}
        that includes Table 1 and any notes that you want the TA to read.
      

  \item  hw.tar.gz that includes all the files specified in
      dropbox/18-19/572/hw4/submit-file-list, plus any source code
      (and binary code) used by the shell scripts.

  \item Make sure that you run {\bf check\_hw4.sh} before
       submitting your hw.tar.gz.
        
    
\end{itemize}

\end{document}



