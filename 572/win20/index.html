<!DOCTYPE html>
<html lang="en">

	<head>

		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
		<meta name="description" content="">
		<meta name="author" content="">

		<title>UW LING 572 (Winter 2020)</title>

		<!-- Bootstrap core CSS -->
		<link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

		<!-- Custom styles for this template -->
		<link href="css/scrolling-nav.css" rel="stylesheet">

	</head>

	<body id="page-top">

		<!-- Navigation -->
		<nav class="navbar navbar-expand-lg navbar-dark bg-dark fixed-top" id="mainNav">
			<div class="container">
				<a class="navbar-brand js-scroll-trigger" href="#page-top">LING 572: Advanced Statistical Methods for NLP [Win '20]</a>
				<button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
					<span class="navbar-toggler-icon"></span>
				</button>
				<div class="collapse navbar-collapse" id="navbarResponsive">
					<ul class="navbar-nav ml-auto">
						<li class="nav-item">
							<a class="nav-link js-scroll-trigger" href="#information">Information</a>
						</li>
						<li class="nav-item">
							<a class="nav-link js-scroll-trigger" href="#policies">Policies</a>
						</li>
						<li class="nav-item">
							<a class="nav-link js-scroll-trigger" href="#schedule">Schedule</a>
						</li>
					</ul>
				</div>
			</div>
		</nav>

		<!--
		<header class="bg-primary text-white">
			<div class="container text-center">
				<h1>Welcome to Scrolling Nav</h1>
				<p class="lead">A landing page template freshly redesigned for Bootstrap 4</p>
			</div>
		</header>
		-->

		<section id="information">
			<div class="container">
				<div class="row">
					<div class="col-lg-12 mx-auto">
						<h2>Course Description</h2>
						<p class="lead">
						This course covers several important machine learning algorithms for natural language processing including decision tree, kNN, Naive Bayes, support vector machine, maximum entropy / multinomial logistic regression, conditional random fields, and neural networks. Students implement many of the algorithms and apply these algorithms to some NLP tasks.
						</p>

						<table class="table">
							<thead>
								<tr>
								<th>Days</th>
								<th>Time</th>
								<th>Location</th>
								</tr>
							</thead>
							<tbody>
								<tr>
									<td>Tuesday and Thursday</td>
									<td>1:00 - 2:20 PM</td>
									<td><a href="https://uw.edu/maps/?ece" target="_blank">ECE</a> 045</td>
								</tr>
							</tbody>
						</table>

						<h2 class="pt-2">Teaching Staff</h2>
						<table class="table">
							<thead>
								<tr>
									<th>Role</th>
									<th>Name</th>
									<th>Office</th>
									<th>Office Hours</th>
								</tr>
							</thead>
							<tbody>
								<tr>
									<td>Instructor</td>
									<td><a href="https://shane.st" target="_blank">Shane Steinert-Threlkeld</a></td>
									<td>Guggenheim 418-D (and Zoom)</td>
									<td>Tuesday, 2:30 - 4:30 PM</td>
								</tr>
								<tr>
									<td>Teaching Assistant</td>
									<td><a href="https://linguistics.washington.edu/people/yuanhe-tian" target="_blank">Yuanhe Tian</a></td>
									<td>Guggenheim 417 (the Treehouse)</td>
									<td>Wednesday, 3 - 4 PM<br />Friday 10 - 11 AM</td>
								</tr>
							</tbody>
						</table>

						<h2 class="pt-2">Textbook</h2>
						<p>There is no required textbook. Instead, the course readings will be drawn from contemporary articles and tutorials available online. Some material will be pulled from the following books, which will be referenced by their parenthecized names:</p>
						<ul>
							<li> [JM] Jurafsky and Martin, <em><a href="https://web.stanford.edu/~jurafsky/slp3/" target="_blank">Speech and Langauge Processing</a></em>, 3rd ed. draft</li>
							<li> [IR] Manning, Raghavan, and Sch&uuml;tze (2008), <em><a href="https://nlp.stanford.edu/IR-book/information-retrieval-book.html" target="_blank">Introduction to Information Retrieval</a></em></li>
							<li> [MS] Manning and Sch&uuml;tze (1999), <em><a href="https://www.cs.vassar.edu/~cs366/docs/Manning_Schuetze_StatisticalNLP.pdf" target="_blank">Foundations of Statistical Natural Language Processing</a></em></li>
							<li> [ML] Alpaydin (2014), <em><a href="https://alliance-primo.hosted.exlibrisgroup.com/permalink/f/kjtuig/CP71211959260001451" target="_blank">Introduction to Machine Learning</a></em></li>
							<li> [DL] Goodfellow, Bengio, and Courville (2016), <em><a href="https://www.deeplearningbook.org/" target="_blank">Deep Learning</a></em></li>
						</ul>


						<h2 class="pt-2">Prerequisites</h2>
						<ul>
							<li>CSE 373 (Data Structures) or Equivalent</li>
							<li>MATH/STAT 394 (Intro to Probability) or Equivalent</li>
							<li>LING 570</li>
							<li>Programming in one or more of Java, Python, C/C++, or Perl</li>
							<li>Linux/Unix Commands</li>
						</ul>

						<h2 class="pt-2">Course Resources</h2>

						<ul>
							<li><a href="https://canvas.uw.edu/courses/1356316" target="_blank">Canvas</a>: lecture recordings, discussion board, homework submission / grading</li>
							<li>Zoom Meeting Room: <a href="https://washington.zoom.us/my/clingzoom" target="_blank">https://washington.zoom.us/my/clingzoom</a></li>
							<li><a href="https://vervet.ling.washington.edu/db/accountrequest-form.php" target="_blank">Patas Account Request</a></li>
							<li><a href="../../571/aut19/welcome_to_patas_1920.pdf" target="_blank">Computing Resources Orientation</a></li>
							<li><a href="https://wiki.ling.washington.edu/bin/view.cgi/Main/CondorClusterHomepage" target="_blank">Condor Wiki Pages</a></li>
						</ul>

						<p><strong>N.B.:</strong> All homework grading will take place on the patas cluster using Condor, so your code must run there.  I strongly encourage you to ensure you have an account set up by the time of the first course meeting.</p>
					</div>
				</div>
			</div>
		</section>

		<section id="policies" class="bg-light">
			<div class="container">
				<div class="row">
					<div class="col-lg-12 mx-auto">
						<h2>Policies</h2>
						<p class="lead">Unless explicitly mentioned below, the <a href="http://depts.washington.edu/uwcl/clms/course-policy.pdf" target="_blank">shared policies of the LING 57x course series</a> apply to this course.  Please read those policies for more information.</p>

						<h2 class="pt-2">Grading</h2>
						<ul>
							<li>100%: Homework Assignments</li>
							<li>Up to 2% adjustment for significant in-class or discussion participation</li>
						</ul>

						<h2 class="pt-2">Communication</h2>
						<p>As per the policy above, all communication outside of the classroom should take place on Canvas.  You can expect responses from teaching staff within 48 hours, but only during normal business hours, and excluding weekends.</p>
						<p><strong>N.B.:</strong> while CLMS students have a private Slack channel, I strongly encourage questions concerning course content and assignments to be posted to the Canvas discussion board, for two reasons.  (i) Teaching staff will not look at Canvas, so misinformation can spread.  (ii) Not every student in the course is in the CLMS program, but they deserve to be included in course discussions and likely have many of the same questions.</p>

						<h2 class="pt-2">Religious Accommodation</h2>
						<p>Washington state law requires that UW develop a policy for accommodation of student absences or significant hardship due to reasons of faith or conscience, or for organized religious activities. The UW’s policy, including more information about how to request an accommodation, is available at <a href="https://registrar.washington.edu/staffandfaculty/religious-accommodations-policy/" target="_blank">Religious Accommodations Policy (https://registrar.washington.edu/staffandfaculty/religious-accommodations-policy/)</a>. Accommodations must be requested within the first two weeks of this course using the <a href="https://registrar.washington.edu/students/religious-accommodations-request/" target="_blank">Religious Accommodations Request form (https://registrar.washington.edu/students/religious-accommodations-request/)</a>.</p>

						<h2 class="pt-2">Access and Accommodations</h2>
						<p>Your experience in this class is important to me. If you have already established accommodations with Disability Resources for Students (DRS), please communicate your approved accommodations to me at your earliest convenience so we can discuss your needs in this course.</p>
						<p>If you have not yet established services through DRS, but have a temporary health condition or permanent disability that requires accommodations (conditions include but not limited to; mental health, attention-related, learning, vision, hearing, physical or health impacts), you are welcome to contact DRS at 206-543-8924 or uwdrs@uw.edu or disability.uw.edu. DRS offers resources and coordinates reasonable accommodations for students with disabilities and/or temporary health conditions.  Reasonable accommodations are established through an interactive process between you, your instructor(s) and DRS.  It is the policy and practice of the University of Washington to create inclusive and accessible learning environments consistent with federal and state law.</p>

						<h2 class="pt-2">Safety</h2>
						<p>Call SafeCampus at 206-685-7233 anytime – no matter where you work or study – to anonymously discuss safety and well-being concerns for yourself or others. SafeCampus’s team of caring professionals will provide individualized support, while discussing short- and long-term solutions and connecting you with additional resources when requested.</p>
					</div>
				</div>
			</div>
		</section>


		<section id="schedule">
			<div class="container">
				<div class="row">
					<div class="col-lg-12 mx-auto">

						<h2>Schedule</h2>

						<br />

						<table class="table">
							<thead>
								<tr>
									<th width="10%">Date</th>
									<th>Topics</th>
									<th width="30%">Suggested Readings</th>
									<th width="15%">Assignment out</th>
									<th width="15%">Assignment due</th>
								</tr>
							</thead>
							<tbody>
								<tr>
									<td>Jan 7</td>
									<td>
										<a href="slides/1_Intro-InformationTheory.pdf" target="_blank">
											Overview<br />
											Information theory
										</a>

										<br />
										<br />
										Background:
										<br />
										[<a href="slides/0_classification.pdf" target="_blank">classification</a>]
										[<a href="slides/0_Probability.pdf" target="_blank">probability</a>]
										[<a href="slides/0_Mallet.pdf" target="_blank">MALLET</a>]
									</td>
									<td>
										MS 2.2
										<br />
										<a href="https://alliance-primo.hosted.exlibrisgroup.com/permalink/f/kjtuig/CP51304963130001451" target="_blank">Cover and Thomas</a>, ch 2
									</td>
									<td>
										HW1 out
										<br />
										[<a href="hw/hw1.pdf" target="_blank">pdf</a>, <a href="hw/hw1.tex" target="_blank">tex</a>]
									</td>
									<td></td>
								</tr>
								<tr>
									<td>Jan 9</td>
									<td><a href="slides/2_DT.pdf" target="_blank">Decision Trees</a></td>
									<td>
										ML ch 9
									</td>
									<td></td>
									<td></td>
								</tr>
								<tr>
									<td>Jan 14</td>
									<td><a href="slides/3_NB.pdf" target="_blank">Naive Bayes</a></td>
									<td>
										IR 13.2-13.4
										<br />
										JM ch 4
										<br />
										<a href="https://www.semanticscholar.org/paper/A-comparison-of-event-models-for-naive-bayes-text-McCallum-Nigam/04ce064505b1635583fa0d9cc07cac7e9ea993cc" target="_blank">McCallum and Nigam 1998</a>
									</td>
									<td>
										HW2 out
										<br />
										[<a href="hw/hw2.pdf" target="_blank">pdf</a>, <a href="hw/hw2.tex" target="_blank">tex</a>, <a href="hw/hw2_slides.pdf" target="_blank">slides</a>]
									</td>
									<td></td>
								</tr>
								<tr>
									<td>Jan 16</td>
									<td><a href="slides/4_kNN.pdf" target="_blank">k-Nearest Neighbors (kNN)</a></td>
									<td>IR 14.3 -14.4</td>
									<td></td>
									<td>HW1 due</td>
								</tr>
								<tr>
									<td>Jan 21</td>
									<td><a href="slides/5_feature-selection.pdf" target="_blank">Feature Selection</a></td>
									<td>IR ch 13.5<br /><a href="https://www.semanticscholar.org/paper/A-Comparative-Study-on-Feature-Selection-in-Text-Yang-Pedersen/c3ebcef26c22a373b6f26a67934213eb0582804e" target="_blank">Yang and Pedersen 1997</a></td>
									<td>
										HW3 out
										<br />
										[<a href="hw/hw3.pdf" target="_blank">pdf</a>, <a href="hw/hw3.tex" target="_blank">tex</a>, <a href="hw/hw3_slides.pdf" target="_blank">slides</a>]
										<br />
										Reading #1 [<a href="hw/reading1.pdf" target="_blank">pdf</a>]
									</td>
									<td></td>
								</tr>
								<tr>
									<td>Jan 23</td>
									<td>
										<a href="slides/6_chi_square.pdf" target="_blank">Chi-square</a>
										<br />
										<a href="slides/6_recap.pdf" target="_blank">Unit 1 recap</a>
									</td>
									<td>IR ch 13.5</td>
									<td></td>
									<td>HW2 due</td>
								</tr>
								<tr>
									<td>Jan 28</td>
									<td>
										<a href="slides/7_optimization.pdf" target="_blank">Optimization</a><br />
										<a href="slides/7_maxent_p1.pdf" target="_blank">Maximum entropy (MaxEnt) I: main concept</a>
									</td>
									<td>
										<a href="https://www.aclweb.org/anthology/J96-1002/" target="_blank">Berger et al 1996</a>, &quot;A maximum entropy approach to NLP&quot;
									</td>
									<td>HW4 out
										<br />
										[<a href="hw/hw4.pdf" target="_blank">pdf</a>, <a href="hw/hw4.tex" target="_blank">tex</a>]
									</td>
									<td>
										Reading #1 in
									</td>
								</tr>
								<tr>
									<td>Jan 30</td>
									<td><a href="slides/8_maxent_p2.pdf" target="_blank">MaxEnt II: modeling and decoding</a></td>
									<td>
										<a href="https://repository.upenn.edu/cgi/viewcontent.cgi?article=1083&context=ircs_reports" target="_blank">Ratnaparkhi 1997</a>
									</td>
									<td></td>
									<td>HW3 in</td>
								</tr>
								<tr>
									<td>Feb 4</td>
									<td>MaxEnt II (cont)</td>
									<td></td>
									<td>HW5 out
										<br />
										[<a href="hw/hw5.pdf" target="_blank">pdf</a>, <a href="hw/hw5.tex" target="_blank">tex</a>, <a href="hw/hw5_slides.pdf" target="_blank">slides</a>]
									</td>
									<td></td>
								</tr>
								<tr>
									<td>Feb 6</td>
									<td>
										<a href="slides/9_maxent_p3.pdf" target="_blank">MaxEnt III: training</a>
										<br />
										<a href="slides/10_maxent_p4.pdf" target="_blank">MaxEnt IV: beam search</a>
									</td>
									<td>
										<a href="https://people.eecs.berkeley.edu/~klein/papers/maxent-tutorial-slides.pdf" target="_blank">Klein and Manning 2003</a>
										<br />
										<a href="https://www.aclweb.org/anthology/W96-0213/" target="_blank">Ratnaparkhi 1996</a>
									</td>
									<td></td>
									<td>HW4 in</td>
								</tr>
								<tr>
									<td>Feb 11</td>
									<td><a href="slides/11_crf.pdf" target="_blank">Conditional Random Fields (CRF)</a></td>
									<td><a href="http://www.cs.umass.edu/~mccallum/papers/crf-tutorial.pdf" target="_blank">Sutton and McCallum 2006</a></td>
									<td>
										HW6 out
										<br />
										<a href="hw/reading2.pdf" target="_blank">Reading #2</a>
									</td>
									<td> </td>
								</tr>
								<tr>
									<td>Feb 13</td>
									<td>Support Vector Machines (SVM) I: linear</td>
									<td>
										IR, Ch 15
									</td>
									<td> </td>
									<td>HW5 in</td>
								</tr>
								<tr>
									<td>Feb 18</td>
									<td>SVM I (cont)</td>
									<td>
										IR, Ch 15
									</td>
									<td>
										Reading #3
									</td>
									<td>
										Reading #2 in
									</td>
								</tr>
								<tr>
									<td>Feb 20</td>
									<td>
										SVM II: non-linear
										<br />
										libSVM
									</td>
									<td>
										IR Ch 15
									</td>
									<td> </td>
									<td>
										HW6 in
									</td>
								</tr>
								<tr>
									<td>Feb 25</td>
									<td>
										SVM III: tree kernel
										<br />
										SVM IV: transductive SVM
									</td>
									<td>
										<a href="http://papers.neurips.cc/paper/2089-convolution-kernels-for-natural-language" target="_blank">Collins and Duffy 2001</a>
										<br />
										<a href="https://www.cs.cornell.edu/people/tj/publications/joachims_99c.pdf" target="_blank">Joachims 1999</a> (ICML)
									</td>
									<td>
										HW7 out
									</td>
									<td>
										Reading #3 in
									</td>
								</tr>
								<tr>
									<td>Feb 27</td>
									<td>Neural Networks: Introduction</td>
									<td>
										DL book <a href="https://www.deeplearningbook.org/contents/mlp.html" target="_blank">ch 6.1-6.4</a>
										<br />
										<a href="https://www.3blue1brown.com/neural-networks" target="_blank">3blue1brown NN videos</a>
									</td>
									<td></td>
									<td></td>
								</tr>
								<tr>
									<td>Mar 3</td>
									<td>Backpropagation</td>
									<td>
										DL book <a href="https://www.deeplearningbook.org/contents/mlp.html" target="_blank">ch 6.5</a>
										<br />
										<a href="http://cs231n.github.io/optimization-2/" target="_blank">CS 231n notes 1</a>
										<br />
										<a href="http://cs231n.stanford.edu/vecDerivs.pdf" target="_blank">CS 231n notes 2 (vector/tensor derivatives)</a>
										<br />
										<a href="https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b" target="_blank">Yes, you should understand backprop</a>
									</td>
									<td>
										HW8 out
									</td>
									<td></td>
								</tr>
								<tr>
									<td>Mar 5</td>
									<td>Recurrent Neural Networks</td>
									<td>
										DL book <a href="https://www.deeplearningbook.org/contents/rnn.html" target="_blank">ch 10</a>
										<br />
										<a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank">The Unreasonable Effectiveness of Recurrent Neural Networks</a>
										<br />
										<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank">Understanding LSTMs</a>
										<br />
										<a href="https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks" target="_blank">Sequence to Sequence Learning with Neural Networks</a> (original seq2seq paper)
										<br />
										<a href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank">Neural Machine Translation by Jointly Learning to Align and Translate</a> (original seq2seq + attention paper)
									</td>
									<td></td>
									<td>
										HW7 in
									</td>
								</tr>
								<tr>
									<td>Mar 10</td>
									<td>Transformers<br />Pre-training / transfer learning</td>
									<td>
										<a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank">Attention is All You Need</a> (original Transformer paper)
										<br />
										<a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html" target="_blank">The Annotated Transformer</a>
										<br />
										<a href="http://jalammar.github.io/illustrated-transformer/" target="_blank">The Illustrated Transformer</a>
										<br />
										<a href="https://thegradient.pub/nlp-imagenet/" target="_blank">NLP's ImageNet Moment Has Arrived</a>
										<br />
										<a href="http://jalammar.github.io/illustrated-bert/">The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)</a>
									</td>
									<td>HW9 out</td>
									<td></td>
								</tr>
								<tr>
									<td>Mar 12</td>
									<td>Summary</td>
									<td></td>
									<td></td>
									<td>HW8 in</td>
								</tr>
							</tbody>
						</table>
					</div>
				</div>
			</div>

		</section>

		<!-- Bootstrap core JavaScript -->
		<script src="vendor/jquery/jquery.min.js"></script>
		<script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

		<!-- Plugin JavaScript -->
		<script src="vendor/jquery-easing/jquery.easing.min.js"></script>

		<!-- Custom JavaScript for this theme -->
		<script src="js/scrolling-nav.js"></script>

		<!-- Google analytics -->
		<script>
			(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
				(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
					m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
			})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

			ga('create', 'UA-6353489-4', 'auto');
			ga('send', 'pageview');
		</script>

	</body>

</html>
