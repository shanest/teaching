<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
		<meta name="description" content="">
		<meta name="author" content="">
		<link rel="icon" href="/docs/4.0/assets/img/favicons/favicon.ico">

		<title>UW LING 571: HW7 (Fall 20222</title>

		<link rel="canonical" href="https://getbootstrap.com/docs/4.0/examples/navbar-fixed/">

		<!-- Bootstrap core CSS -->
		<link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

		<!-- Custom styles for this template -->
		<link href="css/navbar-top-fixed.css" rel="stylesheet">
	</head>

	<body>

		<nav class="navbar navbar-expand-md navbar-dark fixed-top bg-dark">
			<div class="container">
				<a class="navbar-brand" href="index.html">LING 571: Deep Processing Methods for NLP [Aut '22]</a>
				<a class="navbar-brand ml-auto" href="#">Homework 7</a>
				<!--
	    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
	    <span class="navbar-toggler-icon"></span>
	    </button>
	    <div class="collapse navbar-collapse" id="navbarCollapse">
	    <ul class="navbar-nav ml-auto">
	    <li class="nav-item active">
	    <a class="nav-link" href="#">Home <span class="sr-only">(current)</span></a>
	    </li>
	    <li class="nav-item">
	    <a class="nav-link" href="#">Link</a>
	    </li>
	    </ul>
	    </div>
				-->
			</div>
		</nav>

		<main role="main" class="container">
			<p class="lead">This assignment is due <strong>Wednesday, November 16 at 11:59PM</strong>.</p>
			<p><a name="top"></a></p>
			<ol start="1">
				<li><a href="#goals">Goals</a></li>
				<li><a href="#bg">Background</a></li>
				<li><a href="#count">Creating and Evaluating Count-based Models of Distributional Semantic Similarity</a></li>
				<li><a href="#programming">Programming</a></li>
				<li><a href="#compare">Comparison to CBOW</a></li>
				<li><a href="#files">Files</a></li>
			</ol>
			<h2><a name="goals"></a>1. Goals</h2>
			<p>Through this assignment you will:</p>
			<ul>
				<li>Investigate issues and design of distributional semantic models.</li>
				<li>Analyze the effects of different context sizes as well as association measures in distributional similarity models.</li>
				<li>Evaluate distributional models relative to human assessments.</li>
			</ul>
			<p><a href="#top">[Back to Top]</a></p>
			<h2><a name="bg"></a>2. Background</h2>
			<p>Please review the class slides and readings in the textbook on distributional semantics and models. The count-based and word2vec models are to be implemented separately, so that you may do the more extensive coding required for the count-based distributional model in your preferred programming language and then use the Python-based <span style="font-family: courier;"><strong>gensim</strong></span> package for the <span style="font-family: courier;"><strong>word2vec</strong></span> implementation.</p>
			<p><a href="#top">[Back to Top]</a></p>
			<h2><a name="count"></a>3. Creating and Evaluating Count-based Models of Distributional Semantic Similarity</h2>
			<p>Implement a program to create and evaluate a distributional model of word similarity based on local context term cooccurrence. Your program should:</p>
			<ol>
				<li>Read in a corpus that will form the basis of the distributional model and perform basic preprocessing.
					<ul>
						<li>All words should be lowercase.</li>
						<li>Punctuation should removed, both from individual words and from the corpus (e.g. &quot;,&quot; should not be a word). <br />
							Only alphanumeric (a-z, 0-9) characters should remain. <br />
							Note: in many regex packages, including Python's, <span style="font-family: courier">\w</span> matches a single alphanumeric character, while <span style="font-family: courier">\W</span> denotes a single non-alphanumeric character.
						</li>
					</ul>
				</li>
				<li>For each word in the corpus:
					<ul>
						<li>Create a vector representation based on word cooccurrence in a specified window around the word.</li>
						<li>Each element in the vector should receive weight according to a specified weighting</li>
					</ul>
				</li>
				<li>Read in a file of human judgments of similarity between pairs of words. (See Files Section)</li>
				<li>For each word pair in the file:
					<ul>
						<li>For each word in the word pair:
							<ul>
								<li>Print the word and its ten (10) highest weighted features (words) and their weights, in the form:
									<ul>
										<li><span style="font-family: courier;"><strong>word feature1:weight1 feature2:weight2 &#8230;.</strong></span></li>
									</ul>
								</li>
							</ul>
						</li>
						<li>Compute the similarity between the two words, based on <strong>cosine similarity</strong> (e.g. using <span style="font-family: courier">scipy.spatial.distance.cosine</span>, though not that this returns a <em>distance</em>, not a similarity, so read the docs for it carefully).</li>
						<li>Print out the similarity as: <span style="font-family: courier;">wd1,wd2:similarity</span></li>
					</ul>
				</li>
				<li>Lastly, compute and print the Spearman correlation between the similarity scores you have computed and the human-generated similarity scores in the provided file as:<br /><span style="font-family: courier;">correlation:computed_correlation</span>.<br />You may use any available software for computing the correlation. In Python, you can use <span style="font-family: courier;">spearmanr</span> from <span style="font-family: courier;">scipy.stats.stats</span>.</li>
			</ol>
			<p><a href="#top">[Back to Top]</a></p>
			<h2><a name="programming"></a>4. Programming</h2>
			<p>Create a program <strong><span style="font-family: courier;">hw7_dist_similarity.sh</span></strong> that implements the creation and evaluation of the distributional similarity model as described above and invoked as: <br /><strong><span style="font-family: courier;">hw7_dist_similarity.sh &lt;window&gt; &lt;weighting&gt; &lt;judgment_filename&gt; &lt;output_filename&gt;</span></strong>, where:</p>
			<ul>
				<li><strong><span style="font-family: courier;"> &lt;window&gt;</span></strong>
					<ul>
						<li>An integer specifying the size of the context window for your model. For a window value of <strong>2</strong>, the window should span the two words before and the two words after the current word.</li>
					</ul>
				</li>
				<li><strong><span style="font-family: courier;">&lt;weighting&gt;</span></strong>
					<ul>
						<li>A string specifying the weighting scheme to apply: &#8220;FREQ&#8221; or &#8220;PMI&#8221;, where:
							<ul>
								<li><strong>FREQ</strong>: Refers to &#8220;term frequency&#8221;, the number of times the word appeared in the context of the target</li>
								<li><strong>PMI</strong>: (Positive) Point-wise Mutual Information: A variant of PMI where negative association scores are removed.</li>
							</ul>
						</li>
					</ul>
				</li>
				<li><strong><span style="font-family: courier;"> &lt;judgment_filename&gt;</span></strong>
					<ul>
						<li>The name of the input file holding human judgments of the pairs of words and their similarity to evaluate against, mc_similarity.txt.</li>
						<li>Each line is of the form:
							<ul>
								<li><span style="font-family: courier;">wd1,wd2,similarity_score </span></li>
							</ul>
						</li>
					</ul>
				</li>
				<li><strong><span style="font-family: courier;"> &lt;output_filename&gt;</span></strong>:
					<ul>
						<li>The name of the output file with the results of computing similarities and correlations over the word pairs.</li>
						<li>The file name should identify the configuration under which it was run, as in:
							<ul>
								<li><span style="font-family: courier;"><strong>hw7_sim_&lt;window&gt;_&lt;weighting&gt;_output.txt</strong></span></li>
								<li>e.g. <strong><span style="font-family: courier;">hw7_sim_30_FREQ_output.txt</span></strong> would hold the results of running the bag of words model with context window of 30 and term frequency weights.</li>
							</ul>
						</li>
					</ul>
				</li>
			</ul>
			<p>In this assignment, you should use the Brown corpus provided with NLTK in <strong><span style="font-family: courier;">/corpora/nltk/nltk-data/corpora/brown/</span></strong> as the source of cooccurrence information. The file is white-space tokenized, but all tokens are of the form <span style="font-family: courier;"><strong>&#8220;word/POS&#8221;</strong></span>.</p>
			<p>If you choose to use NLTK, you may use the Brown corpus reader as in:<br /><strong><span style="font-family: courier;">brown_words = nltk.corpus.brown.words() </span></strong></p>
			<p><a href="#top">[Back to Top]</a></p>
			<h2><a name="compare"></a>5. Comparison to Continuous Bag of Words (CBOW) using Word2Vec</h2>
			<p>Implement a program to evaluate a predictive CBOW distributional model of word similarity using Word2Vec. Your program should:</p>
			<ol>
				<li>Read in a corpus that will form the basis of the predictive CBOW distributional model and perform basic preprocessing.
					<ul>
						<li>All words should be lowercase.</li>
						<li>Punctuation should removed.</li>
					</ul>
				</li>
				<li>
					Build a continuous bag of words model using a standard implementation package, such as <a href="https://radimrehurek.com/gensim/models/word2vec.html">gensim&#8217;s word2vec</a>
					<br />
					An example call would be: <span style="font-family: courier;">model = gensim.models.word2vec.Word2Vec(sents, vector_size=100, window=2, min_count=1, workers=1)</span>, where <span style="font-family: courier;">sents</span> is a list of lists of tokens.  See the above documentation for more details.
				</li>
				<li>Read in a file of human judgments of similarity between pairs of words.</li>
				<li>For each word pair in the file:
					<ul>
						<li>Compute the similarity between the two words, using the <strong><span style="font-family: courier;">word2vec</span></strong> model</li>
						<li>Print out the similarity as: <strong><span style="font-family: courier;">wd1,wd2:similarity</span></strong></li>
					</ul>
				</li>
				<li>Lastly, compute and print the Spearman correlation between the similarity scores you have computed and the human-generated similarity scores in the provided file as:
					<ul>
						<li><strong><span style="font-family: courier;">correlation:computed_correlation</span></strong>.</li>
						<li>You may use any available software for computing the correlation. In Python, you can use <strong><span style="font-family: courier;">spearmanr</span></strong> from <strong><span style="font-family: courier;">scipy.stats.stats</span></strong>.</li>
					</ul>
				</li>
			</ol>
			<p><strong>NB:</strong> If you want to play with pre-trained embeddings before doing your own training, try:</p>
			<pre>
			import gensim.downloader as api
			wv = api.load('word2vec-google-news-300')
			</pre>
			<p>and have a look at their <a href="https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html" target="_blank">tutorial</a>.</p>
			<p><a href="#top">[Back to Top]</a></p>
			<h3>Programming #2</h3>
			<p>Create a program <strong><span style="font-family: courier;">hw7_cbow_similarity.sh</span></strong> that implements the creation and evaluation of the Continuous Bag-of-Words similarity model as described above and invoked as: <br /><strong><span style="font-family: courier;">hw7_cbow_similarity.sh &lt;window&gt; &lt;judgment_filename&gt; &lt;output_filename&gt;</span></strong>, where:</p>
			<ul>
				<li><strong><span style="font-family: courier;">&lt;window&gt;</span></strong>
					<ul>
						<li>An integer specifying the size of the context window for your model.</li>
						<li>For a window value of <strong>2</strong>, the window should span the two words before and the two words after the current word.</li>
					</ul>
				</li>
				<li><strong><span style="font-family: courier;">&lt;judgment_filename&gt;</span></strong>
					<ul>
						<li>The name of the input file holding human judgments of the pairs of words and their similarity to evaluate against, <strong><span style="font-family: courier;">mc_similarity.txt</span></strong>.
							<ul>
								<li>Each line is of the form: <span style="font-family: courier;">wd1,wd2,similarity_score</span></li>
							</ul>
						</li>
					</ul>
				</li>
				<li><strong><span style="font-family: courier;">&lt;output_filename&gt;</span></strong>
					<ul>
						<li>The name of the output file with the results of computing similarities and correlations over the word pairs. The file name should identify the configuration under which it was run, as in:</li>
						<li><span style="font-family: courier;">hw7_sim_&lt;window&gt;_CBOW_output.txt</span></li>
						<ul>
							<li>e.g. <span style="font-family: courier;">hw7_sim_30_CBOW_output.txt</span> would hold the results of running the Continuous Bag of Words model with context window of <span style="font-family: courier;">30</span>.</li>
						</ul>
					</ul>
				</li>
			</ul>
			<p><a href="#top">[Back to Top]</a></p>
			<h2><a name="files"></a>6. Files</h2>
			<h3>Test and Example Data Files</h3>
			<p>Aside from the Brown corpus, all files related to this assignment may be found on patas in <strong><span style="font-family: courier;">/dropbox/22-23/au571/hw7/</span></strong>, as below:</p>
			<ul>
				<li><strong><span style="font-family: courier;">mc_similarity.txt</span></strong>
					<ul>
						<li>These are the pairs of words whose similarity is to be evaluated under each of your models, along with human similarity judgments from [<a href="https://www.tandfonline.com/doi/abs/10.1080/01690969108406936">Miller and Charles, 1991</a>].</li>
						<li>Each line is of the form:
							<ul>
								<li><span style="font-family: courier;">wd1,wd2,similarity_score </span></li>
							</ul>
						</li>
					</ul>
				</li>
				<li><strong><span style="font-family: courier;">example_similarity_output.txt</span></strong>
					<ul>
						<li>This file holds an example output file with term frequency weights and no pre-processing.</li>
					</ul>
				</li>
			</ul>
			<h3>Submission Files</h3>
			<ul>
				<li><strong><span style="font-family: courier;">hw7.tar.gz</span></strong>: Tarball including the following:
					<ul>
						<li><strong><span style="font-family: courier;"> hw7_dist_similarity.sh</span></strong>: Program which implements and evaluates your count-based distributional similarity model.</li>
						<li><strong><span style="font-family: courier;"> hw7_cbow_similarity.sh</span></strong>: Program which implements and evaluates the word2vec similarity model.</li>
						<li><strong><span style="font-family: courier;"> hw7_sim_2_FREQ_output.txt</span></strong>: Output of running your program with <span style="font-family: courier;">window=2</span> and <span style="font-family: courier;">weighting=FREQ</span>.</li>
						<li><strong><span style="font-family: courier;"> hw7_sim_2_PMI_output.txt</span></strong>: Output of running your program with <span style="font-family: courier;">window=2</span> and <span style="font-family: courier;">weighting=PMI</span>.</li>
						<li><strong><span style="font-family: courier;"> hw7_sim_10_PMI_output.txt</span></strong>: Output of running your program with <span style="font-family: courier;">window=10</span> and <span style="font-family: courier;">weighting=PMI</span>.</li>
						<li><strong><span style="font-family: courier;"> hw7_sim_2_CBOW_output.txt</span></strong>: Output of running your <span style="font-family: courier;">hw7_cbow_similarity.sh</span> program with <span style="font-family: courier;">window=2</span> using word2vec.</li>
					</ul>
				</li>
				<li><span style="font-family: courier;"><strong>readme.{txt|pdf}</strong></span>:
					<ul>
						<li>This file should describe and discuss your work on this assignment. Include problems you came across and how (or if) you were able to solve them, any insights, special features, and what you learned. Give examples if possible. If you were not able to complete parts of the project, discuss what you tried and/or what did not work. This will allow you to receive maximum credit for partial work.
							<p>In particular, you should discuss the effects of window size, weighting, and model on the quality of the similarity model, as captured by the correlation between your automatically calculated similarity and human judgments.</p>
						</li>
					</ul>
				</li>
			</ul>
			<p><a href="#top">[Back to Top]</a></p>

		</main>




		<!-- Bootstrap core JavaScript
	  ================================================== -->
	  <!-- Placed at the end of the document so the pages load faster -->
	  <script src="vendor/jquery/jquery.min.js"></script>
	  <script src="vendor/bootstrap/js/bootstrap.min.js"></script>

		<!-- Goatcounter analytics -->
		<script data-goatcounter="https://shanest.goatcounter.com/count" async src="//gc.zgo.at/count.js"></script>

	</body>
</html>
