Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@inproceedings{Warstadt2019,
abstract = {Though state-of-the-art sentence representation models can perform tasks requiring significant knowledge of grammar, it is an open question how best to evaluate their grammatical knowledge. We explore five experimental methods inspired by prior work evaluating pretrained sentence representation models. We use a single linguistic phenomenon, negative polarity item (NPI) licensing in English, as a case study for our experiments. NPIs like "any" are grammatical only if they appear in a licensing environment like negation ("Sue doesn't have any cats" vs. "Sue has any cats"). This phenomenon is challenging because of the variety of NPI licensing environments that exist. We introduce an artificially generated dataset that manipulates key features of NPI licensing for the experiments. We find that BERT has significant knowledge of these features, but its success varies widely across different experimental methods. We conclude that a variety of methods is necessary to reveal all relevant aspects of a model's grammatical knowledge in a given domain.},
archivePrefix = {arXiv},
arxivId = {1909.02597},
author = {Warstadt, Alex and Cao, Yu and Grosu, Ioana and Peng, Wei and Blix, Hagen and Nie, Yining and Alsop, Anna and Bordia, Shikha and Liu, Haokun and Parrish, Alicia and Wang, Sheng-Fu and Phang, Jason and Mohananey, Anhad and Htut, Phu Mon and Jereti{\v{c}}, Paloma and Bowman, Samuel R.},
booktitle = {Empirical Methods in Natural Language Processing (EMNLP)},
eprint = {1909.02597},
file = {:Users/shanest/Documents/Library/Warstadt et al/Empirical Methods in Natural Language Processing (EMNLP)/Warstadt et al. - 2019 - Investigating BERT's Knowledge of Language Five Analysis Methods with NPIs.pdf:pdf},
title = {{Investigating BERT's Knowledge of Language: Five Analysis Methods with NPIs}},
url = {http://arxiv.org/abs/1909.02597},
year = {2019}
}
@inproceedings{Jumelet2019,
abstract = {Extensive research has recently shown that recurrent neural language models are able to process a wide range of grammatical phenomena. How these models are able to perform these remarkable feats so well, however, is still an open question. To gain more insight into what information LSTMs base their decisions on, we propose a generalisation of Contextual Decomposition (GCD). In particular, this setup enables us to accurately distil which part of a prediction stems from semantic heuristics, which part truly emanates from syntactic cues and which part arise from the model biases themselves instead. We investigate this technique on tasks pertaining to syntactic agreement and co-reference resolution and discover that the model strongly relies on a default reasoning effect to perform these tasks.},
archivePrefix = {arXiv},
arxivId = {1909.08975},
author = {Jumelet, Jaap and Zuidema, Willem and Hupkes, Dieuwke},
booktitle = {Proceedings of the Conference on Computational Natural Language Learning (CoNLL)},
eprint = {1909.08975},
file = {:Users/shanest/Documents/Library/Jumelet, Zuidema, Hupkes/Proceedings of the Conference on Computational Natural Language Learning (CoNLL)/Jumelet, Zuidema, Hupkes - 2019 - Analysing Neural Language Models Contextual Decomposition Reveals Default Reasoning in Number and Gend.pdf:pdf},
title = {{Analysing Neural Language Models: Contextual Decomposition Reveals Default Reasoning in Number and Gender Assignment}},
url = {http://arxiv.org/abs/1909.08975},
year = {2019}
}
@article{McCoy2018,
abstract = {Recurrent neural networks (RNNs) can learn continuous vector representations of symbolic structures such as sequences and sentences; these representations often exhibit linear regularities (analogies). Such regularities motivate our hypothesis that RNNs that show such regularities implicitly compile symbolic structures into tensor product representations (TPRs; Smolensky, 1990), which additively combine tensor products of vectors representing roles (e.g., sequence positions) and vectors representing fillers (e.g., particular words). To test this hypothesis, we introduce Tensor Product Decomposition Networks (TPDNs), which use TPRs to approximate existing vector representations. We demonstrate using synthetic data that TPDNs can successfully approximate linear and tree-based RNN autoencoder representations, suggesting that these representations exhibit interpretable compositional structure; we explore the settings that lead RNNs to induce such structure-sensitive representations. By contrast, further TPDN experiments show that the representations of four models trained to encode naturally-occurring sentences can be largely approximated with a bag-of-words, with only marginal improvements from more sophisticated structures. We conclude that TPDNs provide a powerful method for interpreting vector representations, and that standard RNNs can induce compositional sequence representations that are remarkably well approximated by TPRs; at the same time, existing training tasks for sentence representation learning may not be sufficient for inducing robust structural representations.},
archivePrefix = {arXiv},
arxivId = {1812.08718},
author = {McCoy, R. Thomas and Linzen, Tal and Dunbar, Ewan and Smolensky, Paul},
eprint = {1812.08718},
file = {:Users/shanest/Documents/Library/McCoy et al/Unknown/McCoy et al. - 2018 - RNNs Implicitly Implement Tensor Product Representations.pdf:pdf},
pages = {1--22},
title = {{RNNs Implicitly Implement Tensor Product Representations}},
url = {http://arxiv.org/abs/1812.08718},
year = {2018}
}
@inproceedings{Peters2019,
abstract = {Contextual word representations derived from pre-trained bidirectional language models (biLMs) have recently been shown to provide significant improvements to the state of the art for a wide range of NLP tasks. However, many questions remain as to how and why these models are so effective. In this paper, we present a detailed empirical study of how the choice of neural architecture (e.g. LSTM, CNN, or self attention) influences both end task accuracy and qualitative properties of the representations that are learned. We show there is a tradeoff between speed and accuracy, but all architectures learn high quality contextual representations that outperform word embeddings for four challenging NLP tasks. Additionally, all architectures learn representations that vary with network depth, from exclusively morphological based at the word embedding layer through local syntax based in the lower contextual layers to longer range semantics such coreference at the upper layers. Together, these results suggest that unsupervised biLMs, independent of architecture, are learning much more about the structure of language than previously appreciated.},
address = {Stroudsburg, PA, USA},
author = {Peters, Matthew and Neumann, Mark and Zettlemoyer, Luke and Yih, Wen-tau},
booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
doi = {10.18653/v1/D18-1179},
file = {:Users/shanest/Documents/Library/Peters et al/Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing/Peters et al. - 2018 - Dissecting Contextual Word Embeddings Architecture and Representation.pdf:pdf},
pages = {1499--1509},
publisher = {Association for Computational Linguistics},
title = {{Dissecting Contextual Word Embeddings: Architecture and Representation}},
url = {https://aclweb.org/anthology/D18-1179},
year = {2018}
}
@inproceedings{Conneau2018,
abstract = {Although much effort has recently been devoted to training high-quality sentence embeddings, we still have a poor understanding of what they are capturing. "Downstream" tasks, often based on sentence classification, are commonly used to evaluate the quality of sentence representations. The complexity of the tasks makes it however difficult to infer what kind of information is present in the representations. We introduce here 10 probing tasks designed to capture simple linguistic features of sentences, and we use them to study embeddings generated by three different encoders trained in eight distinct ways, uncovering intriguing properties of both encoders and training methods.},
address = {Stroudsburg, PA, USA},
author = {Conneau, Alexis and Kruszewski, German and Lample, Guillaume and Barrault, Lo{\"{i}}c and Baroni, Marco},
booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
doi = {10.18653/v1/P18-1198},
file = {:Users/shanest/Documents/Library/Conneau et al/Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1 Long Papers)/Conneau et al. - 2018 - What you can cram into a single $&amp!# vector Probing sentence embeddings for linguistic properties.pdf:pdf},
isbn = {9781948087322},
pages = {2126--2136},
publisher = {Association for Computational Linguistics},
title = {{What you can cram into a single $&!#* vector: Probing sentence embeddings for linguistic properties}},
url = {http://aclweb.org/anthology/P18-1198},
volume = {1},
year = {2018}
}
@article{Belinkov2019,
abstract = {The field of natural language processing has seen impressive progress in recent years, with neural network models replacing many of the traditional systems. A plethora of new models have been proposed, many of which are thought to be opaque compared to their feature-rich counterparts. This has led researchers to analyze, interpret, and evaluate neural networks in novel and more fine-grained ways. In this survey paper, we review analysis methods in neural language processing, categorize them according to prominent research trends, highlight existing limitations, and point to potential directions for future work.},
author = {Belinkov, Yonatan and Glass, James},
doi = {10.1162/tacl_a_00254},
file = {:Users/shanest/Documents/Library/Belinkov, Glass/Transactions of the Association for Computational Linguistics/Belinkov, Glass - 2019 - Analysis Methods in Neural Language Processing A Survey.pdf:pdf},
journal = {Transactions of the Association for Computational Linguistics},
pages = {49--72},
title = {{Analysis Methods in Neural Language Processing: A Survey}},
volume = {7},
year = {2019}
}
@unpublished{Devlin2018,
archivePrefix = {arXiv},
arxivId = {1810.04805v1},
author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
eprint = {1810.04805v1},
file = {:Users/shanest/Documents/Library/Devlin et al/Unknown/Devlin et al. - 2018 - BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf:pdf},
title = {{BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}},
year = {2018}
}
@inproceedings{Peters2018,
abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
archivePrefix = {arXiv},
arxivId = {1802.05365},
author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
booktitle = {Proceedings of North American Association for Computational Linguistics (NAACL)},
eprint = {1802.05365},
file = {:Users/shanest/Documents/Library/Peters et al/Proceedings of North American Association for Computational Linguistics (NAACL)/Peters et al. - 2018 - Deep contextualized word representations.pdf:pdf},
month = {feb},
title = {{Deep contextualized word representations}},
url = {http://arxiv.org/abs/1802.05365},
year = {2018}
}
@inproceedings{Coenen2019,
archivePrefix = {arXiv},
arxivId = {1906.02715v2},
author = {Coenen, Andy and Yuan, Ann and Kim, Been and Pearce, Adam and Vi{\'{e}}gas, Fernanda and Wattenberg, Martin},
booktitle = {Proceedings of the 33rd Conference on Neural Information Processing Systems (NeurIPS 2019)},
eprint = {1906.02715v2},
file = {:Users/shanest/Documents/Library/Coenen et al/Proceedings of the 33rd Conference on Neural Information Processing Systems (NeurIPS 2019)/Coenen et al. - 2019 - Visualizing and Measuring the Geometry of BERT.pdf:pdf},
title = {{Visualizing and Measuring the Geometry of BERT}},
year = {2019}
}
@article{Lake2017,
abstract = {Humans can understand and produce new utterances effortlessly, thanks to their systematic compositional skills. Once a person learns the meaning of a new verb "dax," he or she can immediately understand the meaning of "dax twice" or "sing and dax." In this paper, we introduce the SCAN domain, consisting of a set of simple compositional navigation commands paired with the corresponding action sequences. We then test the zero-shot generalization capabilities of a variety of recurrent neural networks (RNNs) trained on SCAN with sequence-to-sequence methods. We find that RNNs can generalize well when the differences between training and test commands are small, so that they can apply "mix-and-match" strategies to solve the task. However, when generalization requires systematic compositional skills (as in the "dax" example above), RNNs fail spectacularly. We conclude with a proof-of-concept experiment in neural machine translation, supporting the conjecture that lack of systematicity is an important factor explaining why neural networks need very large training sets.},
archivePrefix = {arXiv},
arxivId = {1711.00350},
author = {Lake, Brenden M. and Baroni, Marco},
eprint = {1711.00350},
file = {:Users/shanest/Documents/Library/Lake, Baroni/Unknown/Lake, Baroni - 2017 - Still not systematic after all these years On the compositional skills of sequence-to-sequence recurrent networks.pdf:pdf},
pages = {1--12},
title = {{Still not systematic after all these years: On the compositional skills of sequence-to-sequence recurrent networks}},
url = {http://arxiv.org/abs/1711.00350},
year = {2017}
}
@article{Hahn2019,
abstract = {Recurrent neural networks (RNNs) have reached striking performance in many natural language processing tasks. This has renewed interest in whether these generic sequence processing devices are inducing genuine linguistic knowledge. Nearly all current analytical studies, however, initialize the RNNs with a vocabulary of known words, and feed them tokenized input during training. We present a multi-lingual study of the linguistic knowledge encoded in RNNs trained as character-level language models, on input data with word boundaries removed. These networks face a tougher and more cognitively realistic task, having to discover any useful linguistic unit from scratch based on input statistics. The results show that our “near tabula rasa” RNNs are mostly able to solve morphological, syntactic and semantic tasks that intuitively presuppose word-level knowledge, and indeed they learned, to some extent, to track word boundaries. Our study opens the door to speculations about the necessity of an explicit, rigid word lexicon in language learning and usage.},
archivePrefix = {arXiv},
arxivId = {arXiv:1906.07285v1},
author = {Hahn, Michael and Baroni, Marco},
doi = {10.1162/tacl_a_00283},
eprint = {arXiv:1906.07285v1},
file = {:Users/shanest/Documents/Library/Hahn, Baroni/Transactions of the Association for Computational Linguistics/Hahn, Baroni - 2019 - Tabula Nearly Rasa Probing the Linguistic Knowledge of Character-level Neural Language Models Trained on Unsegment.pdf:pdf},
journal = {Transactions of the Association for Computational Linguistics},
pages = {467--484},
title = {{ Tabula Nearly Rasa: Probing the Linguistic Knowledge of Character-level Neural Language Models Trained on Unsegmented Text }},
volume = {7},
year = {2019}
}
@article{Sengupta2014,
abstract = {It has been proposed that the ability of humans to quickly perceive numerosity involves a visual sense of number. Different paradigms of enumeration and numerosity comparison have produced a gamut of behavioral and neuroimaging data, but there has been no unified conceptual framework that can explain results across the entire range of numerosity. The current work tries to address the ongoing debate concerning whether the same mechanism operates for enumeration of small and large numbers, through a computational approach. We describe the workings of a single-layered, fully connected network characterized by self-excitation and recurrent inhibition that operates at both subitizing and estimation ranges. We show that such a network can account for classic numerical cognition effects (the distance effect, Fechner's law, Weber fraction for numerosity comparison) through the network steady state activation response across different recurrent inhibition values. The model also accounts for fMRI data previously reported for different enumeration related tasks. The model also allows us to generate an estimate of the pattern of reaction times in enumeration tasks. Overall, these findings suggest that a single network architecture can account for both small and large number processing.},
author = {Sengupta, Rakesh and Surampudi, Bapi Raju and Melcher, David},
doi = {10.1016/j.brainres.2014.03.014},
file = {:Users/shanest/Documents/Library/Sengupta, Surampudi, Melcher/Brain Research/Sengupta, Surampudi, Melcher - 2014 - A visual sense of number emerges from the dynamics of a recurrent on-center off-surround neural ne.pdf:pdf},
issn = {18726240},
journal = {Brain Research},
keywords = {Computational model On-center off-surround,Enumeration,Neural network,Numerical cognition,Spatial attention Individuation,Visual sense of numbers},
pages = {114--124},
pmid = {25108042},
publisher = {Elsevier},
title = {{A visual sense of number emerges from the dynamics of a recurrent on-center off-surround neural network}},
url = {http://dx.doi.org/10.1016/j.brainres.2014.03.014},
volume = {1582},
year = {2014}
}
@article{Goldberg2019,
abstract = {I assess the extent to which the recently introduced BERT model captures English syntactic phenomena, using (1) naturally-occurring subject-verb agreement stimuli; (2) "coloreless green ideas" subject-verb agreement stimuli, in which content words in natural sentences are randomly replaced with words sharing the same part-of-speech and inflection; and (3) manually crafted stimuli for subject-verb agreement and reflexive anaphora phenomena. The BERT model performs remarkably well on all cases.},
archivePrefix = {arXiv},
arxivId = {1901.05287},
author = {Goldberg, Yoav},
eprint = {1901.05287},
file = {:Users/shanest/Documents/Library/Goldberg/Unknown/Goldberg - 2019 - Assessing BERT's Syntactic Abilities.pdf:pdf},
pages = {2--5},
title = {{Assessing BERT's Syntactic Abilities}},
url = {http://arxiv.org/abs/1901.05287},
year = {2019}
}
@article{Warstadt2018,
abstract = {In this work, we explore the ability of artificial neural networks to judge the grammatical acceptability of a sentence. Machine learning research of this kind is well placed to answer important open questions about the role of prior linguistic bias in language acquisition by providing a test for the Poverty of the Stimulus Argument. In service of this goal, we introduce the Corpus of Linguistic Acceptability (CoLA), a set of 10,657 English sentences labeled as grammatical or ungrammatical by expert linguists. We train several recurrent neural networks to do binary acceptability classification. These models set a baseline for the task. Error-analysis testing the models on specific grammatical phenomena reveals that they learn some systematic grammatical generalizations like subject-verb-object word order without any grammatical supervision. We find that neural sequence models show promise on the acceptability classification task. However, human-like performance across a wide range of grammatical constructions remains far off.},
archivePrefix = {arXiv},
arxivId = {1805.12471},
author = {Warstadt, Alex and Singh, Amanpreet and Bowman, Samuel R.},
eprint = {1805.12471},
file = {:Users/shanest/Documents/Library/Warstadt, Singh, Bowman/Transactions of the Association for Computational Linguistics/Warstadt, Singh, Bowman - 2019 - Neural Network Acceptability Judgments.pdf:pdf},
journal = {Transactions of the Association for Computational Linguistics},
title = {{Neural Network Acceptability Judgments}},
url = {http://arxiv.org/abs/1805.12471},
year = {2019}
}
@article{Stoianov2012,
abstract = {Numerosity estimation is phylogenetically ancient and foundational to human mathematical learning, but its computational bases remain controversial. Here we show that visual numerosity emerges as a statistical property of images in 'deep networks' that learn a hierarchical generative model of the sensory input. Emergent numerosity detectors had response profiles resembling those of monkey parietal neurons and supported numerosity estimation with the same behavioral signature shown by humans and animals.},
author = {Stoianov, Ivilin and Zorzi, Marco},
doi = {10.1038/nn.2996},
file = {:Users/shanest/Documents/Library/Stoianov, Zorzi/Nature Neuroscience/Stoianov, Zorzi - 2012 - Emergence of a `visual number sense' in hierarchical generative models.pdf:pdf;:Users/shanest/Documents/Library/Stoianov, Zorzi/Nature Neuroscience/Stoianov, Zorzi - 2012 - Emergence of a `visual number sense' in hierarchical generative models(2).pdf:pdf},
journal = {Nature Neuroscience},
number = {2},
pages = {194--196},
publisher = {Nature Publishing Group},
title = {{Emergence of a `visual number sense' in hierarchical generative models}},
volume = {15},
year = {2012}
}
@article{Potts2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1809.03068v1},
author = {Potts, Christopher},
eprint = {arXiv:1809.03068v1},
file = {:Users/shanest/Documents/Library/Potts/Unknown/Potts - 2018 - A case for deep learning in semantics.pdf:pdf},
pages = {1--10},
title = {{A case for deep learning in semantics}},
year = {2018}
}
@article{Linzen2019,
author = {Linzen, Tal},
doi = {10.1353/lan.2019.0001},
file = {:Users/shanest/Documents/Library/Linzen/Language/Linzen - 2019 - What can linguistics and deep learning contribute to each other Response to Pater.pdf:pdf},
issn = {1535-0665},
journal = {Language},
keywords = {deep learning,neural networks,poverty of the stimulus,psycholinguistics,syntax},
number = {1},
title = {{What can linguistics and deep learning contribute to each other? Response to Pater}},
url = {https://muse.jhu.edu/article/718440},
volume = {95},
year = {2019}
}
@inproceedings{Yanaka2019,
abstract = {Monotonicity reasoning is one of the important reasoning skills for any intelligent natural language inference (NLI) model in that it requires the ability to capture the interaction between lexical and syntactic structures. Since no test set has been developed for monotonicity reasoning with wide coverage, it is still unclear whether neural models can perform monotonicity reasoning in a proper way. To investigate this issue, we introduce the Monotonicity Entailment Dataset (MED). Performance by state-of-the-art NLI models on the new test set is substantially worse, under 55%, especially on downward reasoning. In addition, analysis using a monotonicity-driven data augmentation method showed that these models might be limited in their generalization ability in upward and downward reasoning.},
address = {Stroudsburg, PA, USA},
author = {Yanaka, Hitomi and Mineshima, Koji and Bekki, Daisuke and Inui, Kentaro and Sekine, Satoshi and Abzianidze, Lasha and Bos, Johan},
booktitle = {Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP},
doi = {10.18653/v1/W19-4804},
file = {:Users/shanest/Documents/Library/Yanaka et al/Proceedings of the 2019 ACL Workshop BlackboxNLP Analyzing and Interpreting Neural Networks for NLP/Yanaka et al. - 2019 - Can Neural Networks Understand Monotonicity Reasoning.pdf:pdf},
pages = {31--40},
publisher = {Association for Computational Linguistics},
title = {{Can Neural Networks Understand Monotonicity Reasoning?}},
url = {https://www.aclweb.org/anthology/W19-4804},
year = {2019}
}
@inproceedings{Wilcox2019,
abstract = {State-of-the-art LSTM language models trained on large corpora learn sequential contingencies in impressive detail and have been shown to acquire a number of non-local grammatical dependencies with some success. Here we investigate whether supervision with hierarchical structure enhances learning of a range of grammatical dependencies, a question that has previously been addressed only for subject-verb agreement. Using controlled experimental methods from psycholinguistics, we compare the performance of word-based LSTM models versus two models that represent hierarchical structure and deploy it in left-to-right processing: Recurrent Neural Network Grammars (RNNGs) (Dyer et al., 2016) and a incrementalized version of the Parsing-as-Language-Modeling configuration from Chariak et al., (2016). Models are tested on a diverse range of configurations for two classes of non-local grammatical dependencies in English---Negative Polarity licensing and Filler--Gap Dependencies. Using the same training data across models, we find that structurally-supervised models outperform the LSTM, with the RNNG demonstrating best results on both types of grammatical dependencies and even learning many of the Island Constraints on the filler--gap dependency. Structural supervision thus provides data efficiency advantages over purely string-based training of neural language models in acquiring human-like generalizations about non-local grammatical dependencies.},
archivePrefix = {arXiv},
arxivId = {1903.00943},
author = {Wilcox, Ethan and Qian, Peng and Futrell, Richard and Ballesteros, Miguel and Levy, Roger},
booktitle = {Proceedings of North American Association for Computational Linguistics (NAACL)},
eprint = {1903.00943},
file = {:Users/shanest/Documents/Library/Wilcox et al/Proceedings of North American Association for Computational Linguistics (NAACL)/Wilcox et al. - 2019 - Structural Supervision Improves Learning of Non-Local Grammatical Dependencies.pdf:pdf},
pages = {3302--3312},
title = {{Structural Supervision Improves Learning of Non-Local Grammatical Dependencies}},
url = {http://arxiv.org/abs/1903.00943},
year = {2019}
}
@article{Weiss2018,
abstract = {While Recurrent Neural Networks (RNNs) are famously known to be Turing complete, this relies on infinite precision in the states and unbounded computation time. We consider the case of RNNs with finite precision whose computation time is linear in the input length. Under these limitations, we show that different RNN variants have different computational power. In particular, we show that the LSTM and the Elman-RNN with ReLU activation are strictly stronger than the RNN with a squashing activation and the GRU. This is achieved because LSTMs and ReLU-RNNs can easily implement counting behavior. We show empirically that the LSTM does indeed learn to effectively use the counting mechanism.},
archivePrefix = {arXiv},
arxivId = {1805.04908},
author = {Weiss, Gail and Goldberg, Yoav and Yahav, Eran},
eprint = {1805.04908},
file = {:Users/shanest/Documents/Library/Weiss, Goldberg, Yahav/Unknown/Weiss, Goldberg, Yahav - 2018 - On the Practical Computational Power of Finite Precision RNNs for Language Recognition.pdf:pdf},
number = {2017},
title = {{On the Practical Computational Power of Finite Precision RNNs for Language Recognition}},
url = {http://arxiv.org/abs/1805.04908},
year = {2018}
}
@inproceedings{OSullivan2019,
abstract = {How are the meanings of linguistic expressions related to their use in concrete cognitive tasks? Visual identification tasks show human speakers can exhibit considerable variation in their understanding, representation and verification of certain quantifiers. This paper initiates an investigation into neural models of these psycho-semantic tasks. We trained two types of network -- a convolutional neural network (CNN) model and a recurrent model of visual attention (RAM) -- on the "most" verification task from \citet{Pietroski2009}, manipulating the visual scene and novel notions of task duration. Our results qualitatively mirror certain features of human performance (such as sensitivity to the ratio of set sizes, indicating a reliance on approximate number) while differing in interesting ways (such as exhibiting a subtly different pattern for the effect of image type). We conclude by discussing the prospects for using neural models as cognitive models of this and other psychosemantic tasks.},
address = {Stroudsburg, PA, USA},
author = {O'Sullivan, Lewis and Steinert-Threlkeld, Shane},
booktitle = {Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics},
doi = {10.18653/v1/W19-2916},
file = {:Users/shanest/Documents/Library/O'Sullivan, Steinert-Threlkeld/Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics/O'Sullivan, Steinert-Threlkeld - 2019 - Neural Models of the Psychosemantics of ‘Most'.pdf:pdf},
number = {2009},
pages = {140--151},
publisher = {Association for Computational Linguistics},
title = {{Neural Models of the Psychosemantics of ‘Most'}},
url = {http://aclweb.org/anthology/W19-2916},
year = {2019}
}
@inproceedings{Tenney2019,
abstract = {Contextualized representation models such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018) have recently achieved state-of-the-art results on a diverse array of downstream NLP tasks. Building on recent token-level probing work, we introduce a novel edge probing task design and construct a broad suite of sub-sentence tasks derived from the traditional structured NLP pipeline. We probe word-level contextual representations from four recent models and investigate how they encode sentence structure across a range of syntactic, semantic, local, and long-range phenomena. We find that existing models trained on language modeling and translation produce strong representations for syntactic phenomena, but only offer comparably small improvements on semantic tasks over a non-contextual baseline.},
archivePrefix = {arXiv},
arxivId = {1905.06316},
author = {Tenney, Ian and Xia, Patrick and Chen, Berlin and Wang, Alex and Poliak, Adam and McCoy, R Thomas and Kim, Najoung and {Van Durme}, Benjamin and Bowman, Samuel R and Das, Dipanjan and Pavlick, Ellie},
booktitle = {International Conference of Learning Representations (ICLR 2019)},
eprint = {1905.06316},
file = {:Users/shanest/Documents/Library/Tenney et al/International Conference of Learning Representations (ICLR 2019)/Tenney et al. - 2019 - What do you learn from context Probing for sentence structure in contextualized word representations.pdf:pdf},
pages = {1--17},
title = {{What do you learn from context? Probing for sentence structure in contextualized word representations}},
url = {http://arxiv.org/abs/1905.06316},
year = {2019}
}
@inproceedings{Marvin2019,
abstract = {We present a dataset for evaluating the grammaticality of the predictions of a language model. We automatically construct a large number of minimally different pairs of English sentences, each consisting of a grammatical and an ungrammatical sentence. The sentence pairs represent different variations of structure-sensitive phenomena: subject-verb agreement, reflexive anaphora and negative polarity items. We expect a language model to assign a higher probability to the grammatical sentence than the ungrammatical one. In an experiment using this data set, an LSTM language model performed poorly on many of the constructions. Multi-task training with a syntactic objective (CCG supertagging) improved the LSTM's accuracy, but a large gap remained between its performance and the accuracy of human participants recruited online. This suggests that there is considerable room for improvement over LSTMs in capturing syntax in a language model.},
address = {Stroudsburg, PA, USA},
author = {Marvin, Rebecca and Linzen, Tal},
booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
doi = {10.18653/v1/D18-1151},
file = {:Users/shanest/Documents/Library/Marvin, Linzen/Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing/Marvin, Linzen - 2018 - Targeted Syntactic Evaluation of Language Models.pdf:pdf},
pages = {1192--1202},
publisher = {Association for Computational Linguistics},
title = {{Targeted Syntactic Evaluation of Language Models}},
url = {http://aclweb.org/anthology/D18-1151},
year = {2018}
}
@article{Futrell2018,
abstract = {Recurrent neural networks (RNNs) are the state of the art in sequence modeling for natural language. However, it remains poorly understood what grammatical characteristics of natural language they implicitly learn and represent as a consequence of optimizing the language modeling objective. Here we deploy the methods of controlled psycholinguistic experimentation to shed light on to what extent RNN behavior reflects incremental syntactic state and grammatical dependency representations known to characterize human linguistic behavior. We broadly test two publicly available long short-term memory (LSTM) English sequence models, and learn and test a new Japanese LSTM. We demonstrate that these models represent and maintain incremental syntactic state, but that they do not always generalize in the same way as humans. Furthermore, none of our models learn the appropriate grammatical dependency configurations licensing reflexive pronouns or negative polarity items.},
archivePrefix = {arXiv},
arxivId = {1809.01329},
author = {Futrell, Richard and Wilcox, Ethan and Morita, Takashi and Levy, Roger},
eprint = {1809.01329},
file = {:Users/shanest/Documents/Library/Futrell et al/Unknown/Futrell et al. - 2018 - RNNs as psycholinguistic subjects Syntactic state and grammatical dependency.pdf:pdf},
title = {{RNNs as psycholinguistic subjects: Syntactic state and grammatical dependency}},
url = {http://arxiv.org/abs/1809.01329},
year = {2018}
}
@inproceedings{An2019a,
abstract = {Neural language models have achieved state-of-the-art performances on many NLP tasks, and recently have been shown to learn a number of hierarchically-sensitive syntactic dependencies between individual words. However, equally important for language processing is the ability to combine words into phrasal constituents, and use constituent-level features to drive downstream expectations. Here we investigate neural models' ability to represent constituent-level features, using coordinated noun phrases as a case study. We assess whether different neural language models trained on English and French represent phrase-level number and gender features, and use those features to drive downstream expectations. Our results suggest that models use a linear combination of NP constituent number to drive CoordNP/verb number agreement. This behavior is highly regular and even sensitive to local syntactic context, however it differs crucially from observed human behavior. Models have less success with gender agreement. Models trained on large corpora perform best, and there is no obvious advantage for models trained using explicit syntactic supervision.},
archivePrefix = {arXiv},
arxivId = {1909.04625},
author = {An, Aixiu and Qian, Peng and Wilcox, Ethan and Levy, Roger},
booktitle = {Empirical Methods in Natural Language Processing (EMNLP)},
eprint = {1909.04625},
file = {:Users/shanest/Documents/Library/An et al/Empirical Methods in Natural Language Processing (EMNLP)/An et al. - 2019 - Representation of Constituents in Neural Language Models Coordination Phrase as a Case Study.pdf:pdf},
title = {{Representation of Constituents in Neural Language Models: Coordination Phrase as a Case Study}},
url = {http://arxiv.org/abs/1909.04625},
year = {2019}
}
@article{Hupkes2018,
abstract = {We investigate how neural networks can learn and process languages with hierarchical, compositional semantics. To this end, we define the artificial task of processing nested arithmetic expressions, and study whether different types of neural networks can learn to compute their meaning. We find that recursive neural networks can find a generalising solution to this problem, and we visualise this solution by breaking it up in three steps: project, sum and squash. As a next step, we investigate recurrent neural networks, and show that a gated recurrent unit, that processes its input incrementally, also performs very well on this task. To develop an understanding of what the recurrent network encodes, visualisation techniques alone do not suffice. Therefore, we develop an approach where we formulate and test multiple hypotheses on the information encoded and processed by the network. For each hypothesis, we derive predictions about features of the hidden state representations at each time step, and train 'diagnostic classifiers' to test those predictions. Our results indicate that the networks follow a strategy similar to our hypothesised 'cumulative strategy', which explains the high accuracy of the network on novel expressions, the generalisation to longer expressions than seen in training, and the mild deterioration with increasing length. This is turn shows that diagnostic classifiers can be a useful technique for opening up the black box of neural networks. We argue that diagnostic classification, unlike most visualisation techniques, does scale up from small networks in a toy domain, to larger and deeper recurrent networks dealing with real-life data, and may therefore contribute to a better understanding of the internal dynamics of current state-of-the-art models in natural language processing.},
archivePrefix = {arXiv},
arxivId = {arXiv:1711.10203v2},
author = {Hupkes, Dieuwke and Veldhoen, Sara and Zuidema, Willem},
eprint = {arXiv:1711.10203v2},
file = {:Users/shanest/Documents/Library/Hupkes, Veldhoen, Zuidema/Journal of Artificial Intelligence Research/Hupkes, Veldhoen, Zuidema - 2018 - Visualisation and `Diagnostic Classifiers' Reveal how Recurrent and Recursive Neural Networks Process.pdf:pdf},
journal = {Journal of Artificial Intelligence Research},
pages = {907--926},
title = {{Visualisation and `Diagnostic Classifiers' Reveal how Recurrent and Recursive Neural Networks Process Hierarchical Structure}},
volume = {61},
year = {2018}
}
@inproceedings{Jumelet2018,
address = {Stroudsburg, PA, USA},
archivePrefix = {arXiv},
arxivId = {arXiv:1808.10627v1},
author = {Jumelet, Jaap and Hupkes, Dieuwke},
booktitle = {Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP},
doi = {10.18653/v1/W18-5424},
eprint = {arXiv:1808.10627v1},
file = {:Users/shanest/Documents/Library/Jumelet, Hupkes/Proceedings of the 2018 EMNLP Workshop BlackboxNLP Analyzing and Interpreting Neural Networks for NLP/Jumelet, Hupkes - 2018 - Do Language Models Understand Anything On the Ability of LSTMs to Understand Negative Polarity Items.pdf:pdf},
pages = {222--231},
publisher = {Association for Computational Linguistics},
title = {{Do Language Models Understand Anything? On the Ability of LSTMs to Understand Negative Polarity Items}},
url = {http://aclweb.org/anthology/W18-5424},
year = {2018}
}
@inproceedings{Giulianelli2018,
abstract = {How do neural language models keep track of number agreement between subject and verb? We show that `diagnostic classifiers', trained to predict number from the internal states of a language model, provide a detailed understanding of how, when, and where this information is represented. Moreover, they give us insight into when and where number information is corrupted in cases where the language model ends up making agreement errors. To demonstrate the causal role played by the representations we find, we then use agreement information to influence the course of the LSTM during the processing of difficult sentences. Results from such an intervention reveal a large increase in the language model's accuracy. Together, these results show that diagnostic classifiers give us an unrivalled detailed look into the representation of linguistic information in neural models, and demonstrate that this knowledge can be used to improve their performance.},
archivePrefix = {arXiv},
arxivId = {1808.08079},
author = {Giulianelli, Mario and Harding, Jack and Mohnert, Florian and Hupkes, Dieuwke and Zuidema, Willem},
booktitle = {Proceedings of the 2018 EMNLP Workshop BlackboxNLP},
eprint = {1808.08079},
file = {:Users/shanest/Documents/Library/Giulianelli et al/Proceedings of the 2018 EMNLP Workshop BlackboxNLP/Giulianelli et al. - 2018 - Under the Hood Using Diagnostic Classifiers to Investigate and Improve how Language Models Track Agreement I.pdf:pdf},
pages = {240--248},
title = {{Under the Hood: Using Diagnostic Classifiers to Investigate and Improve how Language Models Track Agreement Information}},
url = {http://arxiv.org/abs/1808.08079},
year = {2018}
}
@article{Andreas2019,
abstract = {Many machine learning algorithms represent input data with vector embeddings or discrete codes. When inputs exhibit compositional structure (e.g. objects built from parts or procedures from subroutines), it is natural to ask whether this compositional structure is reflected in the the inputs' learned representations. While the assessment of compositionality in languages has received significant attention in linguistics and adjacent fields, the machine learning literature lacks general-purpose tools for producing graded measurements of compositional structure in more general (e.g. vector-valued) representation spaces. We describe a procedure for evaluating compositionality by measuring how well the true representation-producing model can be approximated by a model that explicitly composes a collection of inferred representational primitives. We use the procedure to provide formal and empirical characterizations of compositional structure in a variety of settings, exploring the relationship between compositionality and learning dynamics, human judgments, representational similarity, and generalization.},
archivePrefix = {arXiv},
arxivId = {1902.07181},
author = {Andreas, Jacob},
eprint = {1902.07181},
file = {:Users/shanest/Documents/Library/Andreas/Unknown/Andreas - 2019 - Measuring Compositionality in Representation Learning.pdf:pdf},
pages = {1--15},
title = {{Measuring Compositionality in Representation Learning}},
url = {http://arxiv.org/abs/1902.07181},
year = {2019}
}
@inproceedings{Lake2018,
archivePrefix = {arXiv},
arxivId = {1711.00350v3},
author = {Lake, Brenden and Baroni, Marco},
booktitle = {International Conference of Machine Learning (ICML 2018)},
eprint = {1711.00350v3},
file = {:Users/shanest/Documents/Library/Lake, Baroni/International Conference of Machine Learning (ICML 2018)/Lake, Baroni - 2018 - Generalization without Systematicity On the Compositional Skills of Sequence-to-Sequence Recurrent Networks.pdf:pdf},
title = {{Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks}},
year = {2018}
}
@inproceedings{Niven2019,
abstract = {We are surprised to find that BERT's peak performance of 77% on the Argument Reasoning Comprehension Task reaches just three points below the average untrained human baseline. However, we show that this result is entirely accounted for by exploitation of spurious statistical cues in the dataset. We analyze the nature of these cues and demonstrate that a range of models all exploit them. This analysis informs the construction of an adversarial dataset on which all models achieve random accuracy. Our adversarial dataset provides a more robust assessment of argument comprehension and should be adopted as the standard in future work.},
address = {Stroudsburg, PA, USA},
author = {Niven, Timothy and Kao, Hung-Yu},
booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
doi = {10.18653/v1/P19-1459},
file = {:Users/shanest/Documents/Library/Niven, Kao/Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics/Niven, Kao - 2019 - Probing Neural Network Comprehension of Natural Language Arguments.pdf:pdf},
pages = {4658--4664},
publisher = {Association for Computational Linguistics},
title = {{Probing Neural Network Comprehension of Natural Language Arguments}},
url = {https://www.aclweb.org/anthology/P19-1459},
year = {2019}
}
@inproceedings{Voita2019,
abstract = {We seek to understand how the representations of individual tokens and the structure of the learned feature space evolve between layers in deep neural networks under different learning objectives. We focus on the Transformers for our analysis as they have been shown effective on various tasks, including machine translation (MT), standard left-to-right language models (LM) and masked language modeling (MLM). Previous work used black-box probing tasks to show that the representations learned by the Transformer differ significantly depending on the objective. In this work, we use canonical correlation analysis and mutual information estimators to study how information flows across Transformer layers and how this process depends on the choice of learning objective. For example, as you go from bottom to top layers, information about the past in left-to-right language models gets vanished and predictions about the future get formed. In contrast, for MLM, representations initially acquire information about the context around the token, partially forgetting the token identity and producing a more generalized token representation. The token identity then gets recreated at the top MLM layers.},
archivePrefix = {arXiv},
arxivId = {1909.01380},
author = {Voita, Elena and Sennrich, Rico and Titov, Ivan},
booktitle = {Empirical Methods in Natural Language Processing (EMNLP)},
eprint = {1909.01380},
file = {:Users/shanest/Documents/Library/Voita, Sennrich, Titov/Empirical Methods in Natural Language Processing (EMNLP)/Voita, Sennrich, Titov - 2019 - The Bottom-up Evolution of Representations in the Transformer A Study with Machine Translation and Langu.pdf:pdf},
title = {{The Bottom-up Evolution of Representations in the Transformer: A Study with Machine Translation and Language Modeling Objectives}},
url = {http://arxiv.org/abs/1909.01380},
year = {2019}
}
@inproceedings{Hewitt2019,
abstract = {Recent work has improved our ability to detect linguistic knowledge in word representations. However, current methods for detecting syntactic knowledge do not test whether syntax trees are represented in their entirety. In this work, we propose a structural probe, which evaluates whether syntax trees are embedded in a linear transformation of a neural network's word representation space. The probe identifies a linear transformation under which squared L2 distance encodes the distance between words in the parse tree, and one in which squared L2 norm encodes depth in the parse tree. Using our probe, we show that such transformations exist for both ELMo and BERT but not in baselines, providing evidence that entire syntax trees are embedded implicitly in deep models' vector geometry.},
address = {Stroudsburg, PA, USA},
author = {Hewitt, John and Manning, Christopher D.},
booktitle = {Proceedings of the 2019 Conference of the North},
doi = {10.18653/v1/N19-1419},
file = {:Users/shanest/Documents/Library/Hewitt, Manning/Proceedings of the 2019 Conference of the North/Hewitt, Manning - 2019 - A Structural Probe for Finding Syntax in Word Representations.pdf:pdf},
pages = {4129--4138},
publisher = {Association for Computational Linguistics},
title = {{A Structural Probe for Finding Syntax in Word Representations}},
url = {https://github.com/ http://aclweb.org/anthology/N19-1419},
year = {2019}
}
@inproceedings{Evans2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1802.08535v1},
author = {Evans, Richard and Saxton, David and Amos, David and Kohli, Pushmeet and Grefenstette, Edward},
booktitle = {International Conference of Learning Representations},
eprint = {arXiv:1802.08535v1},
file = {:Users/shanest/Documents/Library/Evans et al/International Conference of Learning Representations/Evans et al. - 2018 - Can Neural Networks Understand Logical Entailment.pdf:pdf},
title = {{Can Neural Networks Understand Logical Entailment}},
year = {2018}
}
@article{Weiss2017,
abstract = {We address the problem of extracting an automaton from a trained recurrent neural network (RNN). We present a novel algorithm that uses exact learning and abstract interpretation to perform efficient extraction of a minimal automaton describing the state dynamics of a given RNN. We use Angluin's L* algorithm as a learner and the given RNN as an oracle, employing abstract interpretation of the RNN for answering equivalence queries. Our technique allows automaton-extraction from the RNN while avoiding state explosion, even when the state vectors are large and fine differentiation is required between RNN states. We experiment with automata extraction from multi-layer GRU and LSTM based RNNs, with state-vector dimensions and underlying automata and alphabet sizes which are significantly larger than in previous automata-extraction work. In some cases, the underlying target language can be described with a succinct automata, yet the extracted automata is large and complex. These are cases in which the RNN failed to learn the intended generalization, and our extraction procedure highlights words which are misclassified by the seemingly "perfect" RNN.},
archivePrefix = {arXiv},
arxivId = {1711.09576},
author = {Weiss, Gail and Goldberg, Yoav and Yahav, Eran},
eprint = {1711.09576},
file = {:Users/shanest/Documents/Library/Weiss, Goldberg, Yahav/Unknown/Weiss, Goldberg, Yahav - 2017 - Extracting Automata from Recurrent Neural Networks Using Queries and Counterexamples.pdf:pdf},
title = {{Extracting Automata from Recurrent Neural Networks Using Queries and Counterexamples}},
url = {http://arxiv.org/abs/1711.09576},
year = {2017}
}
@inproceedings{White2018,
abstract = {We investigate neural models' ability to capture lexicosyntactic inferences: inferences triggered by the interaction of lexical and syntactic information. We take the task of event factuality prediction as a case study and build a factuality judgment dataset for all English clause-embedding verbs in various syntactic contexts. We use this dataset, which we make publicly available, to probe the behavior of current state-of-the-art neural systems, showing that these systems make certain systematic errors that are clearly visible through the lens of factuality prediction.},
archivePrefix = {arXiv},
arxivId = {1808.06232},
author = {White, Aaron Steven and Rudinger, Rachel and Rawlins, Kyle and {Van Durme}, Benjamin},
booktitle = {Empirical Methods in Natural Language Processing (EMNLP)},
eprint = {1808.06232},
file = {:Users/shanest/Documents/Library/White et al/Empirical Methods in Natural Language Processing (EMNLP)/White et al. - 2018 - Lexicosyntactic Inference in Neural Models.pdf:pdf},
title = {{Lexicosyntactic Inference in Neural Models}},
year = {2018}
}
@article{Russin2019,
abstract = {Standard methods in deep learning for natural language processing fail to capture the compositional structure of human language that allows for systematic generalization outside of the training distribution. However, human learners readily generalize in this way, e.g. by applying known grammatical rules to novel words. Inspired by work in neuroscience suggesting separate brain systems for syntactic and semantic processing, we implement a modification to standard approaches in neural machine translation, imposing an analogous separation. The novel model, which we call Syntactic Attention, substantially outperforms standard methods in deep learning on the SCAN dataset, a compositional generalization task, without any hand-engineered features or additional supervision. Our work suggests that separating syntactic from semantic learning may be a useful heuristic for capturing compositional structure.},
archivePrefix = {arXiv},
arxivId = {1904.09708},
author = {Russin, Jake and Jo, Jason and O'Reilly, Randall C.},
eprint = {1904.09708},
file = {:Users/shanest/Documents/Library/Russin, Jo, O'Reilly/Unknown/Russin, Jo, O'Reilly - 2019 - Compositional generalization in a deep seq2seq model by separating syntax and semantics.pdf:pdf},
title = {{Compositional generalization in a deep seq2seq model by separating syntax and semantics}},
url = {http://arxiv.org/abs/1904.09708},
year = {2019}
}
@article{Linzen2016,
abstract = {The success of long short-term memory (LSTM) neural networks in language processing is typically attributed to their ability to capture long-distance statistical regularities. Linguistic regularities are often sensitive to syntactic structure; can such dependencies be captured by LSTMs, which do not have explicit structural representations? We begin addressing this question using number agreement in English subject-verb dependencies. We probe the architecture's grammatical competence both using training objectives with an explicit grammatical target (number prediction, grammaticality judgments) and using language models. In the strongly supervised settings, the LSTM achieved very high overall accuracy (less than 1% errors), but errors increased when sequential and structural information conflicted. The frequency of such errors rose sharply in the language-modeling setting. We conclude that LSTMs can capture a non-trivial amount of grammatical structure given targeted supervision, but stronger architectures may be required to further reduce errors; furthermore, the language modeling signal is insufficient for capturing syntax-sensitive dependencies, and should be supplemented with more direct supervision if such dependencies need to be captured.},
archivePrefix = {arXiv},
arxivId = {1611.01368},
author = {Linzen, Tal and Dupoux, Emmanuel and Goldberg, Yoav},
eprint = {1611.01368},
file = {:Users/shanest/Documents/Library/Linzen, Dupoux, Goldberg/Transactions of the Association for Computational Linguistics/Linzen, Dupoux, Goldberg - 2016 - Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies.pdf:pdf},
journal = {Transactions of the Association for Computational Linguistics},
pages = {521--535},
title = {{Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies}},
url = {http://arxiv.org/abs/1611.01368},
volume = {4},
year = {2016}
}
@article{Gulordava2018,
abstract = {Recurrent neural networks (RNNs) have achieved impressive results in a variety of linguistic processing tasks, suggesting that they can induce non-trivial properties of language. We investigate here to what extent RNNs learn to track abstract hierarchical syntactic structure. We test whether RNNs trained with a generic language modeling objective in four languages (Italian, English, Hebrew, Russian) can predict long-distance number agreement in various constructions. We include in our evaluation nonsensical sentences where RNNs cannot rely on semantic or lexical cues ("The colorless green ideas I ate with the chair sleep furiously"), and, for Italian, we compare model performance to human intuitions. Our language-model-trained RNNs make reliable predictions about long-distance agreement, and do not lag much behind human performance. We thus bring support to the hypothesis that RNNs are not just shallow-pattern extractors, but they also acquire deeper grammatical competence.},
archivePrefix = {arXiv},
arxivId = {1803.11138},
author = {Gulordava, Kristina and Bojanowski, Piotr and Grave, Edouard and Linzen, Tal and Baroni, Marco},
eprint = {1803.11138},
file = {:Users/shanest/Documents/Library/Gulordava et al/Unknown/Gulordava et al. - 2018 - Colorless green recurrent networks dream hierarchically.pdf:pdf},
title = {{Colorless green recurrent networks dream hierarchically}},
url = {http://arxiv.org/abs/1803.11138},
year = {2018}
}
@inproceedings{Liu2019,
abstract = {Contextual word representations derived from large-scale neural language models are successful across a diverse set of NLP tasks, suggesting that they encode useful and transferable features of language. To shed light on the linguistic knowledge they capture, we study the representations produced by several recent pretrained contextualizers (variants of ELMo, the OpenAI transformer language model, and BERT) with a suite of seventeen diverse probing tasks. We find that linear models trained on top of frozen contextual representations are competitive with state-of-the-art task-specific models in many cases, but fail on tasks requiring fine-grained linguistic knowledge (e.g., conjunct identification). To investigate the transferability of contextual word representations, we quantify differences in the transferability of individual layers within contextualizers, especially between recurrent neural networks (RNNs) and transformers. For instance, higher layers of RNNs are more task-specific, while transformer layers do not exhibit the same monotonic trend. In addition, to better understand what makes contextual word representations transferable, we compare language model pretraining with eleven supervised pretraining tasks. For any given task, pretraining on a closely related task yields better performance than language model pretraining (which is better on average) when the pretraining dataset is fixed. However, language model pretraining on more data gives the best results.},
address = {Stroudsburg, PA, USA},
archivePrefix = {arXiv},
arxivId = {1903.08855},
author = {Liu, Nelson F. and Gardner, Matt and Belinkov, Yonatan and Peters, Matthew E. and Smith, Noah A.},
booktitle = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
doi = {10.18653/v1/N19-1112},
eprint = {1903.08855},
file = {:Users/shanest/Documents/Library/Liu et al/Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics Human Language Technologie./Liu et al. - 2019 - Linguistic Knowledge and Transferability of Contextual Representations.pdf:pdf},
pages = {1073--1094},
publisher = {Association for Computational Linguistics},
title = {{Linguistic Knowledge and Transferability of Contextual Representations}},
url = {http://arxiv.org/abs/1903.08855 http://aclweb.org/anthology/N19-1112},
year = {2019}
}
@article{Linzen2018,
author = {Linzen, Tal and Leonard, Brian},
file = {:Users/shanest/Documents/Library/Linzen, Leonard/CogSci/Linzen, Leonard - 2018 - Distinct patterns of syntactic agreement errors in recurrent networks and humans.pdf:pdf},
journal = {CogSci},
keywords = {agreement attraction,psycholinguistics,recurrent neural net-,syntax,works},
number = {Figure 1},
title = {{Distinct patterns of syntactic agreement errors in recurrent networks and humans}},
year = {2018}
}
@article{Sennhauser2018,
abstract = {While long short-term memory (LSTM) neural net architectures are designed to capture sequence information, human language is generally composed of hierarchical structures. This raises the question as to whether LSTMs can learn hierarchical structures. We explore this question with a well-formed bracket prediction task using two types of brackets modeled by an LSTM. Demonstrating that such a system is learnable by an LSTM is the first step in demonstrating that the entire class of CFLs is also learnable. We observe that the model requires exponential memory in terms of the number of characters and embedded depth, where a sub-linear memory should suffice. Still, the model does more than memorize the training input. It learns how to distinguish between relevant and irrelevant information. On the other hand, we also observe that the model does not generalize well. We conclude that LSTMs do not learn the relevant underlying context-free rules, suggesting the good overall performance is attained rather by an efficient way of evaluating nuisance variables. LSTMs are a way to quickly reach good results for many natural language tasks, but to understand and generate natural language one has to investigate other concepts that can make more direct use of natural language's structural nature.},
archivePrefix = {arXiv},
arxivId = {1811.02611},
author = {Sennhauser, Luzi and Berwick, Robert C.},
eprint = {1811.02611},
file = {:Users/shanest/Documents/Library/Sennhauser, Berwick/Unknown/Sennhauser, Berwick - 2018 - Evaluating the Ability of LSTMs to Learn Context-Free Grammars.pdf:pdf},
title = {{Evaluating the Ability of LSTMs to Learn Context-Free Grammars}},
url = {http://arxiv.org/abs/1811.02611},
year = {2018}
}
@article{Veldhoen2016,
abstract = {We investigate how neural networks can be used for hierarchical, compositional semantics. To this end, we define the simple but nontrivial artificial task of pro-cessing nested arithmetic expressions and study whether different types of neural networks can learn to add and subtract. We find that recursive neural networks can implement a generalising solution, and we visualise the intermediate steps: projection, summation and squashing. We also show that gated recurrent neural networks, which process the expressions incrementally, perform surprisingly well on this task: they learn to predict the outcome of the arithmetic expressions with reasonable accuracy, although performance deteriorates with increasing length. To analyse what strategy the recurrent network applies, visualisation techniques are less insightful. Therefore, we develop an approach where we formulate and test hypotheses on what strategies these networks might be following. For each hypoth-esis, we derive predictions about features of the hidden state representations at each time step, and train 'diagnostic classifiers' to test those predictions. Our results indicate the networks follow a strategy similar to our hypothesised 'incremental strategy'.},
author = {Veldhoen, Sara and Hupkes, Dieuwke and Zuidema, Willem},
file = {:Users/shanest/Documents/Library/Veldhoen, Hupkes, Zuidema/CEUR Workshop Proceedings/Veldhoen, Hupkes, Zuidema - 2016 - Diagnostic classifiers Revealing how neural networks process hierarchical structure.pdf:pdf},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
title = {{Diagnostic classifiers: Revealing how neural networks process hierarchical structure}},
volume = {1773},
year = {2016}
}
@inproceedings{Bolukbasi2016,
abstract = {The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between between the words receptionist and female, while maintaining desired associations such as between the words queen and female. We define metrics to quantify both direct and indirect gender biases in embeddings, and develop algorithms to "debias" the embedding. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.},
archivePrefix = {arXiv},
arxivId = {1607.06520},
author = {Bolukbasi, Tolga and Chang, Kai-Wei and Zou, James and Saligrama, Venkatesh and Kalai, Adam},
booktitle = {Advances in {Neural} {Information} {Processing} {Systems} ({NIPS} 29)},
eprint = {1607.06520},
file = {:Users/shanest/Documents/Library/Bolukbasi et al/Unknown/Bolukbasi et al. - 2016 - Man is to Computer Programmer as Woman is to Homemaker Debiasing Word Embeddings(2).pdf:pdf;:Users/shanest/Documents/Library/Bolukbasi et al/Advances in {Neural} {Information} {Processing} {Systems} ({NIPS} 29)/Bolukbasi et al. - 2016 - Man is to Computer Programmer as Woman is to Homemaker Debiasing Word Embeddings.pdf:pdf},
isbn = {978-1-5108-3881-9},
issn = {10495258},
title = {{Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings}},
url = {https://papers.nips.cc/paper/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings.pdf},
year = {2016}
}
@inproceedings{Chrupaa2019,
abstract = {Analysis methods which enable us to better understand the representations and functioning of neural models of language are increasingly needed as deep learning becomes the dominant approach in NLP. Here we present two methods based on Representational Similarity Analysis (RSA) and Tree Kernels (TK) which allow us to directly quantify how strongly the information encoded in neural activation patterns corresponds to information represented by symbolic structures such as syntax trees. We first validate our methods on the case of a simple synthetic language for arithmetic expressions with clearly defined syntax and semantics, and show that they exhibit the expected pattern of results. We then apply our methods to correlate neural representations of English sentences with their constituency parse trees.},
address = {Stroudsburg, PA, USA},
author = {Chrupa{\l}a, Grzegorz and Alishahi, Afra},
booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
doi = {10.18653/v1/P19-1283},
file = {:Users/shanest/Documents/Library/Chrupa{\l}a, Alishahi/Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics/Chrupa{\l}a, Alishahi - 2019 - Correlating Neural and Symbolic Representations of Language.pdf:pdf},
pages = {2952--2962},
publisher = {Association for Computational Linguistics},
title = {{Correlating Neural and Symbolic Representations of Language}},
url = {https://www.aclweb.org/anthology/P19-1283},
year = {2019}
}
@article{Pater2019,
abstract = {The birthdate of both generative linguistics and neural networks can be taken as 1957, the year of the publication of foundational work by both Noam Chomsky and Frank Rosenblatt. This paper traces the development of these two approaches to cognitive science, from their largely autonomous early development in their first thirty years, through their collision in the 1980s around the past tense debate (Rumelhart and McClelland 1986, Pinker and Prince 1988), and their integration in much subsequent work up to the present, 2017. Although this integration has produced a considerable body of results, the continued general gulf between these two lines of research is likely impeding progress in both: on learning in generative linguistics, and on the representation of language in neural modeling. The paper concludes with a brief argument that generative linguistics is unlikely to fulfill its promise of accounting for language learning if it continues to maintain its distance from neural and other statistical approaches to learning.},
author = {Pater, Joe},
doi = {10.1353/lan.2019.0005},
file = {:Users/shanest/Documents/Library/Pater/Language/Pater - 2019 - Generative linguistics and neural networks at 60 Foundation, friction, and fusion.pdf:pdf},
journal = {Language},
number = {1},
title = {{Generative linguistics and neural networks at 60: Foundation, friction, and fusion}},
volume = {95},
year = {2019}
}
@article{Monner2012,
abstract = {Fodor and Pylyshyn [(1988). Connectionism and cognitive architecture: A critical analysis. Cognition, 28(1-2), 3-71] famously argued that neural networks cannot behave systematically short of implementing a combinatorial symbol system. A recent response from Frank et al. [(2009). Connectionist semantic systematicity. Cognition, 110(3), 358-379] claimed to have trained a neural network to behave systematically without implementing a symbol system and without any in-built predisposition towards combinatorial representations. We believe systems like theirs may in fact implement a symbol system on a deeper and more interesting level: one where the symbols are latent - not visible at the level of network structure. In order to illustrate this possibility, we demonstrate our own recurrent neural network that learns to understand sentence-level language in terms of a scene. We demonstrate our model's learned understanding by testing it on novel sentences and scenes. By paring down our model into an architecturally minimal version, we demonstrate how it supports combinatorial computation over distributed representations by using the associative memory operations of Vector Symbolic Architectures. Knowledge of the model's memory scheme gives us tools to explain its errors and construct superior future models. We show how the model designs and manipulates a latent symbol system in which the combinatorial symbols are patterns of activation distributed across the layers of a neural network, instantiating a hybrid of classical symbolic and connectionist representations that combines advantages of both. {\textcopyright} 2012 Copyright Taylor and Francis Group, LLC.},
author = {Monner, Derek and Reggia, James A.},
doi = {10.1080/09540091.2013.798262},
file = {:Users/shanest/Documents/Library/Monner, Reggia/Connection Science/Monner, Reggia - 2012 - Emergent latent symbol systems in recurrent neural networks.pdf:pdf},
issn = {09540091},
journal = {Connection Science},
keywords = {associative memory,distributed symbolic representation,latent symbol system,long short-term memory,recurrent neural network},
number = {4},
pages = {193--225},
title = {{Emergent latent symbol systems in recurrent neural networks}},
volume = {24},
year = {2012}
}
@article{Baroni2019,
abstract = {In the last decade, deep artificial neural networks have achieved astounding performance in many natural language processing tasks. Given the high productivity of language, these models must possess effective generalization abilities. It is widely assumed that humans handle linguistic productivity by means of algebraic compositional rules: Are deep networks similarly compositional? After reviewing the main innovations characterizing current deep language processing networks, I discuss a set of studies suggesting that deep networks are capable of subtle grammar-dependent generalizations, but also that they do not rely on systematic compositional rules. I argue that the intriguing behaviour of these devices (still awaiting a full understanding) should be of interest to linguists and cognitive scientists, as it offers a new perspective on possible computational strategies to deal with linguistic productivity beyond rule-based compositionality, and it might lead to new insights into the less systematic generalization patterns that also appear in natural language.},
archivePrefix = {arXiv},
arxivId = {1904.00157},
author = {Baroni, Marco},
eprint = {1904.00157},
file = {:Users/shanest/Documents/Library/Baroni/Unknown/Baroni - 2019 - Linguistic generalization and compositionality in modern artificial neural networks.pdf:pdf},
keywords = {artificial neural networks,compositionality,deep learning,linguistic produc-,tivity},
pages = {0--1},
title = {{Linguistic generalization and compositionality in modern artificial neural networks}},
url = {http://arxiv.org/abs/1904.00157},
year = {2019}
}
@article{Kirov2018,
abstract = {Can advances in NLP help advance cognitive modeling? We examine the role of artificial neural networks, the current state of the art in many common NLP tasks, by returning to a classic case study. In 1986, Rumelhart and McClelland famously introduced a neural architecture that learned to transduce English verb stems to their past tense forms. Shortly thereafter, Pinker & Prince (1988) presented a comprehensive rebuttal of many of Rumelhart and McClelland's claims. Much of the force of their attack centered on the empirical inadequacy of the Rumelhart and McClelland (1986) model. Today, however, that model is severely outmoded. We show that the Encoder-Decoder network architectures used in modern NLP systems obviate most of Pinker and Prince's criticisms without requiring any simplication of the past tense mapping problem. We suggest that the empirical performance of modern networks warrants a re-examination of their utility in linguistic and cognitive modeling.},
archivePrefix = {arXiv},
arxivId = {1807.04783},
author = {Kirov, Christo and Cotterell, Ryan},
eprint = {1807.04783},
file = {:Users/shanest/Documents/Library/Kirov, Cotterell/Transactions of the Association of Computational Linguistics/Kirov, Cotterell - 2018 - Recurrent Neural Networks in Linguistic Theory Revisiting Pinker and Prince (1988) and the Past Tense Debate.pdf:pdf},
journal = {Transactions of the Association of Computational Linguistics},
pages = {651--665},
title = {{Recurrent Neural Networks in Linguistic Theory: Revisiting Pinker and Prince (1988) and the Past Tense Debate}},
url = {http://arxiv.org/abs/1807.04783},
volume = {6},
year = {2018}
}
