@inproceedings{andreasGoodEnoughCompositionalData2020,
  title = {Good-{{Enough Compositional Data Augmentation}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Andreas, Jacob},
  editor = {Jurafsky, Dan and Chai, Joyce and Schluter, Natalie and Tetreault, Joel},
  year = {2020},
  month = jul,
  pages = {7556--7566},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.acl-main.676},
  urldate = {2024-03-18},
  abstract = {We propose a simple data augmentation protocol aimed at providing a compositional inductive bias in conditional and unconditional sequence models. Under this protocol, synthetic training examples are constructed by taking real training examples and replacing (possibly discontinuous) fragments with other fragments that appear in at least one similar environment. The protocol is model-agnostic and useful for a variety of tasks. Applied to neural sequence-to-sequence models, it reduces error rate by as much as 87\% on diagnostic tasks from the SCAN dataset and 16\% on a semantic parsing task. Applied to n-gram language models, it reduces perplexity by roughly 1\% on small corpora in several languages.},
  file = {/Users/shanest/sync/library/Andreas/2020/Andreas - 2020 - Good-Enough Compositional Data Augmentation.pdf}
}

@inproceedings{andreasMeasuringCompositionalityRepresentation2019,
  title = {Measuring {{Compositionality}} in {{Representation Learning}}},
  booktitle = {International {{Conference}} of {{Learning Representations}}},
  author = {Andreas, Jacob},
  year = {2019},
  eprint = {1902.07181},
  url = {https://openreview.net/forum?id=HJz05o0qK7},
  abstract = {Many machine learning algorithms represent input data with vector embeddings or discrete codes. When inputs exhibit compositional structure (e.g. objects built from parts or procedures from subroutines), it is natural to ask whether this compositional structure is reflected in the the inputs' learned representations. While the assessment of compositionality in languages has received significant attention in linguistics and adjacent fields, the machine learning literature lacks general-purpose tools for producing graded measurements of compositional structure in more general (e.g. vector-valued) representation spaces. We describe a procedure for evaluating compositionality by measuring how well the true representation-producing model can be approximated by a model that explicitly composes a collection of inferred representational primitives. We use the procedure to provide formal and empirical characterizations of compositional structure in a variety of settings, exploring the relationship between compositionality and learning dynamics, human judgments, representational similarity, and generalization.},
  archiveprefix = {arxiv},
  keywords = {method: tree reconstruction error,phenomenon: compositionality},
  file = {/Users/shanest/sync/library/Andreas/2019/Andreas - 2019 - Measuring Compositionality in Representation Learn.pdf}
}

@article{brightonLanguageEvolutionarySystem2005,
  title = {Language as an Evolutionary System},
  author = {Brighton, Henry and Smith, Kenny and Kirby, Simon},
  year = {2005},
  month = sep,
  journal = {Physics of Life Reviews},
  volume = {2},
  number = {3},
  pages = {177--226},
  issn = {1571-0645},
  doi = {10.1016/j.plrev.2005.06.001},
  urldate = {2024-03-18},
  abstract = {John Maynard Smith and E{\"o}rs Szathm{\'a}ry argued that human language signified the eighth major transition in evolution: human language marked a new form of information transmission from one generation to another [Maynard Smith J, Szathm{\'a}ry E. The major transitions in evolution. Oxford: Oxford Univ. Press; 1995]. According to this view language codes cultural information and as such forms the basis for the evolution of complexity in human culture. In this article we develop the theory that language also codes information in another sense: languages code information on their own structure. As a result, languages themselves provide information that influences their own survival. To understand the consequences of this theory we discuss recent computational models of linguistic evolution. Linguistic evolution is the process by which languages themselves evolve. This article draws together this recent work on linguistic evolution and highlights the significance of this process in understanding the evolution of linguistic complexity. Our conclusions are that: (1) the process of linguistic transmission constitutes the basis for an evolutionary system, and (2), that this evolutionary system is only superficially comparable to the process of biological evolution.},
  keywords = {adaptation,Adaptation,artificial life,Artificial life,culture,Culture,evolution,Evolution,language,Language,replication,Replication},
  file = {/Users/shanest/sync/library/Brighton et al/2005/Brighton et al. - 2005 - Language as an evolutionary system.pdf;/Users/shanest/Zotero/storage/NFULIKPP/S1571064505000229.html}
}

@inproceedings{chaabouniCompositionalityGeneralizationEmergent2020,
  title = {Compositionality and {{Generalization In Emergent Languages}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Chaabouni, Rahma and Kharitonov, Eugene and Bouchacourt, Diane and Dupoux, Emmanuel and Baroni, Marco},
  editor = {Jurafsky, Dan and Chai, Joyce and Schluter, Natalie and Tetreault, Joel},
  year = {2020},
  month = jul,
  pages = {4427--4442},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.acl-main.407},
  urldate = {2024-03-18},
  abstract = {Natural language allows us to refer to novel composite concepts by combining expressions denoting their parts according to systematic rules, a property known as compositionality. In this paper, we study whether the language emerging in deep multi-agent simulations possesses a similar ability to refer to novel primitive combinations, and whether it accomplishes this feat by strategies akin to human-language compositionality. Equipped with new ways to measure compositionality in emergent languages inspired by disentanglement in representation learning, we establish three main results: First, given sufficiently large input spaces, the emergent language will naturally develop the ability to refer to novel composite concepts. Second, there is no correlation between the degree of compositionality of an emergent language and its ability to generalize. Third, while compositionality is not necessary for generalization, it provides an advantage in terms of language transmission: The more compositional a language is, the more easily it will be picked up by new learners, even when the latter differ in architecture from the original agents. We conclude that compositionality does not arise from simple generalization pressure, but if an emergent language does chance upon it, it will be more likely to survive and thrive.},
  file = {/Users/shanest/sync/library/Chaabouni et al/2020/Chaabouni et al. - 2020 - Compositionality and Generalization In Emergent La3.pdf}
}

@inproceedings{drozdovCompositionalSemanticParsing2022,
  title = {Compositional {{Semantic Parsing}} with {{Large Language Models}}},
  booktitle = {The {{Eleventh International Conference}} on {{Learning Representations}}},
  author = {Drozdov, Andrew and Sch{\"a}rli, Nathanael and Aky{\"u}rek, Ekin and Scales, Nathan and Song, Xinying and Chen, Xinyun and Bousquet, Olivier and Zhou, Denny},
  year = {2022},
  month = sep,
  url = {https://openreview.net/forum?id=gJW8hSGBys8},
  urldate = {2024-03-19},
  abstract = {Humans can reason compositionally when presented with new tasks. Previous research shows that appropriate prompting techniques enable large language models (LLMs) to solve artificial compositional generalization tasks such as SCAN. In this work, we identify additional challenges in more realistic semantic parsing tasks with larger vocabulary and refine these prompting techniques to address them. Our best method is based on least-to-most prompting: it decomposes the problem using prompting-based syntactic parsing, then uses this decomposition to select appropriate exemplars and to sequentially generate the semantic parse. This method allows us to set a new state of the art for CFQ while requiring only 1\% of the training data used by traditional approaches. Due to the general nature of our approach, we expect similar efforts will lead to new results in other tasks and domains, especially for knowledge-intensive applications.},
  langid = {english},
  file = {/Users/shanest/sync/library/Drozdov et al/2022/Drozdov et al. - 2022 - Compositional Semantic Parsing with Large Language.pdf}
}

@inproceedings{dziriFaithFateLimits2023,
  title = {Faith and {{Fate}}: {{Limits}} of {{Transformers}} on {{Compositionality}}},
  shorttitle = {Faith and {{Fate}}},
  booktitle = {Thirty-Seventh {{Conference}} on {{Neural Information Processing Systems}}},
  author = {Dziri, Nouha and Lu, Ximing and Sclar, Melanie and Li, Xiang Lorraine and Jiang, Liwei and Lin, Bill Yuchen and Welleck, Sean and West, Peter and Bhagavatula, Chandra and Bras, Ronan Le and Hwang, Jena D. and Sanyal, Soumya and Ren, Xiang and Ettinger, Allyson and Harchaoui, Zaid and Choi, Yejin},
  year = {2023},
  month = nov,
  url = {https://openreview.net/forum?id=Fkckkr3ya8&referrer=%5Bthe%20profile%20of%20Xiang%20Ren%5D(%2Fprofile%3Fid%3D~Xiang_Ren1)},
  urldate = {2024-03-18},
  abstract = {Transformer large language models (LLMs) have sparked admiration for their exceptional performance on tasks that demand intricate multi-step reasoning. Yet, these models simultaneously show failures on surprisingly trivial problems. This begs the question: Are these errors incidental, or do they signal more substantial limitations? In an attempt to demystify transformer LLMs, we investigate the limits of these models across three representative compositional tasks---multi-digit multiplication, logic grid puzzles, and a classic dynamic programming problem. These tasks require breaking problems down into sub-steps and synthesizing these steps into a precise answer. We formulate compositional tasks as computation graphs to systematically quantify the level of complexity, and break down reasoning steps into intermediate sub-procedures. Our empirical findings suggest that transformer LLMs solve compositional tasks by reducing multi-step compositional reasoning into linearized subgraph matching, without necessarily developing systematic problem-solving skills. To round off our empirical study, we provide theoretical arguments on abstract multi-step reasoning problems that highlight how autoregressive generations' performance can rapidly decay with increased task complexity.},
  langid = {english},
  file = {/Users/shanest/sync/library/Dziri et al/2023/Dziri et al. - 2023 - Faith and Fate Limits of Transformers on Composit.pdf}
}

@article{fodorConnectionismCognitiveArchitecture1988,
  title = {Connectionism and {{Cognitive Architecture}}},
  author = {Fodor, Jerry and Pylyshyn, Zenon W.},
  year = {1988},
  journal = {Cognition},
  volume = {28},
  number = {1-2},
  pages = {3--71},
  doi = {10.1016/0010-0277(88)90031-5},
  urldate = {2011-10-31},
  abstract = {This paper explores differences between Connectionist proposals for cognitive architecture and the sorts of models that have traditionally been assumed in cognitive science. We claim that the major distinction is that, while both Connectionist and Classical architectures postulate representational mental states, the latter but not the former are committed to a symbol-level of representation, or to a `language of thought': i.e., to representational states that have combinatorial syntactic and semantic structure. Several arguments for combinatorial structure in mental representations are then reviewed. These include arguments based on the `systematicity' of mental representation: i.e., on the fact that cognitive capacities always exhibit certain symmetries, so that the ability to entertain a given thought implies the ability to entertain thoughts with semantically related contents. We claim that such arguments make a powerful case that mind/brain architecture is not Connectionist at the cognitive level. We then consider the possibility that Connectionism may provide an account of the neural (or `abstract neurological') structures in which Classical cognitive architecture is implemented. We survey a number of the standard arguments that have been offered in favor of Connectionism, and conclude that they are coherent only on this interpretation.},
  file = {/Users/shanest/sync/library/Fodor_Pylyshyn/1988/Fodor and Pylyshyn - 1988 - Connectionism and Cognitive Architecture.pdf}
}

@article{hupkesCompositionalityDecomposedHow2020,
  title = {Compositionality {{Decomposed}}: {{How}} Do {{Neural Networks Generalise}}?},
  shorttitle = {Compositionality {{Decomposed}}},
  author = {Hupkes, Dieuwke and Dankers, Verna and Mul, Mathijs and Bruni, Elia},
  year = {2020},
  month = apr,
  journal = {Journal of Artificial Intelligence Research},
  volume = {67},
  pages = {757--795},
  issn = {1076-9757},
  doi = {10.1613/jair.1.11674},
  urldate = {2024-03-18},
  abstract = {Despite a multitude of empirical studies, little consensus exists on whether neural networks are able to generalise compositionally, a controversy that, in part, stems from a lack of agreement about what it means for a neural model to be compositional. As a response to this controversy, we present a set of tests that provide a bridge between, on the one hand, the vast amount of linguistic and philosophical theory about compositionality of language and, on the other, the successful neural models of language. We collect different interpretations of compositionality and translate them into five theoretically grounded tests for models that are formulated on a task-independent level. In particular, we provide tests to investigate (i) if models systematically recombine known parts and rules (ii) if models can extend their predictions beyond the length they have seen in the training data (iii) if models' composition operations are local or global (iv) if models' predictions are robust to synonym substitutions and (v) if models favour rules or exceptions during training. To demonstrate the usefulness of this evaluation paradigm, we instantiate these five tests on a highly compositional data set which we dub PCFG SET and apply the resulting tests to three popular sequence-to-sequence models: a recurrent, a convolution-based and a transformer model. We provide an in-depth analysis of the results, which uncover the strengths and weaknesses of these three architectures and point to potential areas of improvement.},
  copyright = {Copyright (c)},
  langid = {english},
  keywords = {machine learning,natural language,neural networks,rule learning},
  file = {/Users/shanest/sync/library/Hupkes et al/2020/Hupkes et al. - 2020 - Compositionality Decomposed How do Neural Network.pdf}
}

@article{hupkesTaxonomyReviewGeneralization2023,
  title = {A Taxonomy and Review of Generalization Research in {{NLP}}},
  author = {Hupkes, Dieuwke and Giulianelli, Mario and Dankers, Verna and Artetxe, Mikel and Elazar, Yanai and Pimentel, Tiago and Christodoulopoulos, Christos and Lasri, Karim and Saphra, Naomi and Sinclair, Arabella and Ulmer, Dennis and Schottmann, Florian and Batsuren, Khuyagbaatar and Sun, Kaiser and Sinha, Koustuv and Khalatbari, Leila and Ryskina, Maria and Frieske, Rita and Cotterell, Ryan and Jin, Zhijing},
  year = {2023},
  month = oct,
  journal = {Nature Machine Intelligence},
  volume = {5},
  number = {10},
  pages = {1161--1174},
  publisher = {Nature Publishing Group},
  issn = {2522-5839},
  doi = {10.1038/s42256-023-00729-y},
  urldate = {2024-03-18},
  abstract = {The ability to generalize well is one of the primary desiderata for models of natural language processing (NLP), but what `good generalization' entails and how it should be evaluated is not well understood. In this Analysis we present a taxonomy for characterizing and understanding generalization research in NLP. The proposed taxonomy is based on an extensive literature review and contains five axes along which generalization studies can differ: their main motivation, the type of generalization they aim to solve, the type of data shift they consider, the source by which this data shift originated, and the locus of the shift within the NLP modelling pipeline. We use our taxonomy to classify over 700 experiments, and we use the results to present an in-depth analysis that maps out the current state of generalization research in NLP and make recommendations for which areas deserve attention in the future.},
  copyright = {2023 The Author(s)},
  langid = {english},
  keywords = {Computer science,Language and linguistics},
  file = {/Users/shanest/sync/library/Hupkes et al/2023/Hupkes et al. - 2023 - A taxonomy and review of generalization research i.pdf}
}

@inproceedings{keysersMeasuringCompositionalGeneralization2019,
  title = {Measuring {{Compositional Generalization}}: {{A Comprehensive Method}} on {{Realistic Data}}},
  shorttitle = {Measuring {{Compositional Generalization}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Keysers, Daniel and Sch{\"a}rli, Nathanael and Scales, Nathan and Buisman, Hylke and Furrer, Daniel and Kashubin, Sergii and Momchev, Nikola and Sinopalnikov, Danila and Stafiniak, Lukasz and Tihon, Tibor and Tsarkov, Dmitry and Wang, Xiao and van Zee, Marc and Bousquet, Olivier},
  year = {2019},
  month = sep,
  url = {https://openreview.net/forum?id=SygcCnNKwr},
  urldate = {2024-03-18},
  abstract = {State-of-the-art machine learning methods exhibit limited compositional generalization. At the same time, there is a lack of realistic benchmarks that comprehensively measure this ability, which makes it challenging to find and evaluate improvements. We introduce a novel method to systematically construct such benchmarks by maximizing compound divergence while guaranteeing a small atom divergence between train and test sets, and we quantitatively compare this method to other approaches for creating compositional generalization benchmarks. We present a large and realistic natural language question answering dataset that is constructed according to this method, and we use it to analyze the compositional generalization ability of three machine learning architectures. We find that they fail to generalize compositionally and that there is a surprisingly strong negative correlation between compound divergence and accuracy. We also demonstrate how our method can be used to create new compositionality benchmarks on top of the existing SCAN dataset, which confirms these findings.},
  langid = {english},
  file = {/Users/shanest/sync/library/Keysers et al/2019/Keysers et al. - 2019 - Measuring Compositional Generalization A Comprehe.pdf}
}

@inproceedings{kimCOGSCompositionalGeneralization2020,
  title = {{{COGS}}: {{A Compositional Generalization Challenge Based}} on {{Semantic Interpretation}}},
  shorttitle = {{{COGS}}},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Kim, Najoung and Linzen, Tal},
  editor = {Webber, Bonnie and Cohn, Trevor and He, Yulan and Liu, Yang},
  year = {2020},
  month = nov,
  pages = {9087--9105},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.emnlp-main.731},
  urldate = {2024-03-18},
  abstract = {Natural language is characterized by compositionality: the meaning of a complex expression is constructed from the meanings of its constituent parts. To facilitate the evaluation of the compositional abilities of language processing architectures, we introduce COGS, a semantic parsing dataset based on a fragment of English. The evaluation portion of COGS contains multiple systematic gaps that can only be addressed by compositional generalization; these include new combinations of familiar syntactic structures, or new combinations of familiar words and familiar structures. In experiments with Transformers and LSTMs, we found that in-distribution accuracy on the COGS test set was near-perfect (96--99\%), but generalization accuracy was substantially lower (16--35\%) and showed high sensitivity to random seed (+-6--8\%). These findings indicate that contemporary standard NLP models are limited in their compositional generalization capacity, and position COGS as a good way to measure progress.},
  file = {/Users/shanest/Zotero/storage/GRNZJHYS/Kim and Linzen - 2020 - COGS A Compositional Generalization Challenge Bas.pdf}
}

@article{kirbyCompressionCommunicationCultural2015,
  title = {Compression and Communication in the Cultural Evolution of Linguistic Structure},
  author = {Kirby, Simon and Tamariz, Monica and Cornish, Hannah and Smith, Kenny},
  year = {2015},
  journal = {Cognition},
  volume = {141},
  pages = {87--102},
  publisher = {Elsevier B.V.},
  issn = {00100277},
  doi = {10.1016/j.cognition.2015.03.016},
  keywords = {cultural transmission},
  file = {/Users/shanest/sync/library/Kirby et al/2015/Kirby et al. - 2015 - Compression and communication in the cultural evol.pdf}
}

@article{kirbyCumulativeCulturalEvolution2008,
  title = {Cumulative Cultural Evolution in the Laboratory: An Experimental Approach to the Origins of Structure in Human Language.},
  author = {Kirby, Simon and Cornish, Hannah and Smith, Kenny},
  year = {2008},
  journal = {Proceedings of the National Academy of Sciences of the United States of America},
  volume = {105},
  number = {31},
  pages = {10681--10686},
  issn = {0027-8424},
  doi = {10.1073/pnas.0707835105},
  abstract = {We introduce an experimental paradigm for studying the cumulative cultural evolution of language. In doing so we provide the first experimental validation for the idea that cultural transmission can lead to the appearance of design without a designer. Our experiments involve the iterated learning of artificial languages by human participants. We show that languages transmitted culturally evolve in such a way as to maximize their own transmissibility: over time, the languages in our experiments become easier to learn and increasingly structured. Furthermore, this structure emerges purely as a consequence of the transmission of language over generations, without any intentional design on the part of individual language learners. Previous computational and mathematical models suggest that iterated learning provides an explanation for the structure of human language and link particular aspects of linguistic structure with particular constraints acting on language during its transmission. The experimental work presented here shows that the predictions of these models, and models of cultural evolution more generally, can be tested in the laboratory.},
  isbn = {1068110686},
  pmid = {18667697},
  file = {/Users/shanest/sync/library/Kirby et al/2008/Kirby et al. - 2008 - Cumulative cultural evolution in the laboratory a.pdf}
}

@inproceedings{lakeGeneralizationSystematicityCompositional2018,
  title = {Generalization without {{Systematicity}}: {{On}} the {{Compositional Skills}} of {{Sequence-to-Sequence Recurrent Networks}}},
  shorttitle = {Generalization without {{Systematicity}}},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {Lake, Brenden and Baroni, Marco},
  year = {2018},
  month = jul,
  pages = {2873--2882},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v80/lake18a.html},
  urldate = {2024-03-18},
  abstract = {Humans can understand and produce new utterances effortlessly, thanks to their compositional skills. Once a person learns the meaning of a new verb "dax," he or she can immediately understand the meaning of "dax twice" or "sing and dax." In this paper, we introduce the SCAN domain, consisting of a set of simple compositional navigation commands paired with the corresponding action sequences. We then test the zero-shot generalization capabilities of a variety of recurrent neural networks (RNNs) trained on SCAN with sequence-to-sequence methods. We find that RNNs can make successful zero-shot generalizations when the differences between training and test commands are small, so that they can apply "mix-and-match" strategies to solve the task. However, when generalization requires systematic compositional skills (as in the "dax" example above), RNNs fail spectacularly. We conclude with a proof-of-concept experiment in neural machine translation, suggesting that lack of systematicity might be partially responsible for neural networks' notorious training data thirst.},
  langid = {english},
  file = {/Users/shanest/sync/library/Lake_Baroni/2018/Lake and Baroni - 2018 - Generalization without Systematicity On the Compo2.pdf;/Users/shanest/sync/library/Lake_Baroni/2018/Lake and Baroni - 2018 - Generalization without Systematicity On the Compo3.pdf}
}

@article{lakeHumanlikeSystematicGeneralization2023a,
  title = {Human-like Systematic Generalization through a Meta-Learning Neural Network},
  author = {Lake, Brenden M. and Baroni, Marco},
  year = {2023},
  month = nov,
  journal = {Nature},
  volume = {623},
  number = {7985},
  pages = {115--121},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/s41586-023-06668-3},
  urldate = {2024-03-18},
  abstract = {The power of human language and thought arises from systematic compositionality---the algebraic ability to understand and produce novel combinations from known components. Fodor and Pylyshyn1 famously argued that artificial neural networks lack this capacity and are therefore not viable models of the mind. Neural networks have advanced considerably in the years since, yet the systematicity challenge persists. Here we successfully address Fodor and Pylyshyn's challenge by providing evidence that neural networks can achieve human-like systematicity when optimized for their compositional skills. To do so, we introduce the meta-learning for compositionality (MLC) approach for guiding training through a dynamic stream of compositional tasks. To compare humans and machines, we conducted human behavioural experiments using an~instruction learning paradigm. After considering seven different models, we found that, in contrast to perfectly systematic but rigid probabilistic symbolic models, and perfectly flexible but unsystematic neural networks, only MLC achieves both the systematicity and flexibility needed for human-like generalization. MLC also advances the compositional skills of machine learning systems in several systematic generalization benchmarks. Our results show how a standard neural network architecture, optimized for its compositional skills, can mimic human systematic generalization in a head-to-head comparison.},
  copyright = {2023 The Author(s)},
  langid = {english},
  keywords = {Computer science,Human behaviour},
  file = {/Users/shanest/sync/library/Lake_Baroni/2023/Lake and Baroni - 2023 - Human-like systematic generalization through a met.pdf}
}

@inproceedings{liSLOGStructuralGeneralization2023,
  title = {{{SLOG}}: {{A Structural Generalization Benchmark}} for {{Semantic Parsing}}},
  shorttitle = {{{SLOG}}},
  booktitle = {Proceedings of the 2023 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Li, Bingzhi and Donatelli, Lucia and Koller, Alexander and Linzen, Tal and Yao, Yuekun and Kim, Najoung},
  editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
  year = {2023},
  month = dec,
  pages = {3213--3232},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.emnlp-main.194},
  urldate = {2024-03-18},
  abstract = {The goal of compositional generalization benchmarks is to evaluate how well models generalize to new complex linguistic expressions. Existing benchmarks often focus on lexical generalization, the interpretation of novel lexical items in syntactic structures familiar from training; structural generalization tasks, where a model needs to interpret syntactic structures that are themselves unfamiliar from training, are often underrepresented, resulting in overly optimistic perceptions of how well models can generalize. We introduce SLOG, a semantic parsing dataset that extends COGS (Kim and Linzen, 2020) with 17 structural generalization cases. In our experiments, the generalization accuracy of Transformer models, including pretrained ones, only reaches 40.6\%, while a structure-aware parser only achieves 70.8\%. These results are far from the near-perfect accuracy existing models achieve on COGS, demonstrating the role of SLOG in foregrounding the large discrepancy between models' lexical and structural generalization capacities.},
  file = {/Users/shanest/sync/library/Li et al/2023/Li et al. - 2023 - SLOG A Structural Generalization Benchmark for Se.pdf}
}

@article{paginCompositionalityDefinitionsVariants2010,
  title = {Compositionality {{I}}: {{Definitions}} and {{Variants}}.},
  author = {Pagin, Peter and Westerst{\aa}hl, Dag},
  year = {2010},
  journal = {Philosophy Compass},
  volume = {5},
  number = {3},
  pages = {250--264},
  doi = {10.1111/j.1747-9991.2009.00228.x},
  abstract = {This is the first part of a two-part article on semantic compositionality, that is, the principle that the meaning of a complex expression is determined by the meanings of its parts and the way they are put together. Here we provide a brief historical background, a formal framework for syntax and semantics, precise definitions, and a survey of variants of compositionality. Stronger and weaker forms are distinguished, as well as generalized forms that cover extra-linguistic context dependence as well as linguistic context dependence. In the second article, we survey arguments for and arguments against the claim that natural languages are compositional, and consider some problem cases. It will be referred to as Part II.},
  file = {/Users/shanest/sync/library/Pagin_Westerstr a hl/2010/Pagin and Westerstr a hl - 2010 - Compositionality I Definitions and Variants..pdf}
}

@article{paginCompositionalityIIArguments2010,
  title = {Compositionality {{II}}: {{Arguments}} and {{Problems}}.},
  author = {Pagin, Peter and Westerst{\aa}hl, Dag},
  year = {2010},
  journal = {Philosophy Compass},
  volume = {5},
  number = {3},
  pages = {265--282},
  doi = {10.1111/j.1747-9991.2009.00229.x},
  abstract = {This is the second part of a two-part article on compositionality, i.e. the principle that the meaning of a complex expression is determined by the meanings of its parts and the way they are put together. In the first, Pagin and Westerst{\aa}hl (2010), we provide a general historical background, a formal framework, definitions, and a survey of variants of compositionality. It will be referred to as Part I. Here we discuss arguments for and against the claim that natural languages have a compositional semantics. We also discuss some problem cases, including belief reports, quotation, idioms, and ambiguity.},
  file = {/Users/shanest/sync/library/Pagin_Westerstr a hl/2010/Pagin and Westerstr a hl - 2010 - Compositionality II Arguments and Problems..pdf}
}

@article{pottsCaseDeepLearning2019,
  title = {A Case for Deep Learning in Semantics: {{Response}} to {{Pater}}},
  shorttitle = {A Case for Deep Learning in Semantics},
  author = {Potts, Christopher},
  year = {2019},
  journal = {Language},
  volume = {95},
  number = {1},
  pages = {e115-e124},
  publisher = {Linguistic Society of America},
  issn = {1535-0665},
  doi = {10.1353/lan.2019.0019},
  urldate = {2024-03-18},
  abstract = {Pater's (2019) target article builds a persuasive case for establishing stronger ties between theoretical linguistics and connectionism (deep learning). This commentary extends his arguments to semantics, focusing in particular on issues of learning, compositionality, and lexical meaning.*},
  keywords = {compositionality,connectionism,deep learning,lexical semantics,machine learning,position,semantics,survey},
  file = {/Users/shanest/sync/library/Potts/2019/Potts - 2019 - A case for deep learning in semantics Response to2.pdf}
}

@article{ravivRoleSocialNetwork2020,
  title = {The {{Role}} of {{Social Network Structure}} in the {{Emergence}} of {{Linguistic Structure}}},
  author = {Raviv, Limor and Meyer, Antje and {Lev-Ari}, Shiri},
  year = {2020},
  journal = {Cognitive Science},
  volume = {44},
  number = {8},
  issn = {15516709},
  doi = {10.1111/cogs.12876},
  abstract = {Social network structure has been argued to shape the structure of languages, as well as affect the spread of innovations and the formation of conventions in the community. Specifically, theoretical and computational models of language change predict that sparsely connected communities develop more systematic languages, while tightly knit communities can maintain high levels of linguistic complexity and variability. However, the role of social network structure in the cultural evolution of languages has never been tested experimentally. Here, we present results from a behavioral group communication study, in which we examined the formation of new languages created in the lab by micro-societies that varied in their network structure. We contrasted three types of social networks: fully connected, small-world, and scale-free. We examined the artificial languages created by these different networks with respect to their linguistic structure, communicative success, stability, and convergence. Results did not reveal any effect of network structure for any measure, with all languages becoming similarly more systematic, more accurate, more stable, and more shared over time. At the same time, small-world networks showed the greatest variation in their convergence, stabilization, and emerging structure patterns, indicating that network structure can influence the community's susceptibility to random linguistic changes (i.e., drift).},
  pmid = {32808326},
  keywords = {Grammatical structure,Input variability,Language evolution,Linguistic diversity,Network structure,Social structure},
  file = {/Users/shanest/sync/library/Raviv et al/2020/Raviv et al. - 2020 - The Role of Social Network Structure in the Emerge.pdf}
}

@inproceedings{reymondMSCANDatasetMultilingual2023,
  title = {{{mSCAN}}: {{A Dataset}} for {{Multilingual Compositional Generalisation Evaluation}}},
  shorttitle = {{{mSCAN}}},
  booktitle = {Proceedings of the 1st {{GenBench Workshop}} on ({{Benchmarking}}) {{Generalisation}} in {{NLP}}},
  author = {Reymond, Am{\'e}lie and {Steinert-Threlkeld}, Shane},
  editor = {Hupkes, Dieuwke and Dankers, Verna and Batsuren, Khuyagbaatar and Sinha, Koustuv and Kazemnejad, Amirhossein and Christodoulopoulos, Christos and Cotterell, Ryan and Bruni, Elia},
  year = {2023},
  month = dec,
  pages = {143--151},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  url = {https://aclanthology.org/2023.genbench-1.11},
  urldate = {2023-12-14},
  abstract = {Language models achieve remarkable results on a variety of tasks, yet still struggle on compositional generalisation benchmarks. The majority of these benchmarks evaluate performance in English only, leaving us with the question of whether these results generalise to other languages. As an initial step to answering this question, we introduce mSCAN, a multilingual adaptation of the SCAN dataset. It was produced by a rule-based translation, developed in cooperation with native speakers. We then showcase this novel dataset on some in-context learning experiments, and GPT3.5 and the multilingual large language model BLOOM},
  file = {/Users/shanest/Zotero/storage/W7ZFR6TU/Reymond and Steinert-Threlkeld - 2023 - mSCAN A Dataset for Multilingual Compositional Ge.pdf}
}

@inproceedings{ruisBenchmarkSystematicGeneralization2020,
  title = {A {{Benchmark}} for {{Systematic Generalization}} in {{Grounded Language Understanding}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Ruis, Laura and Andreas, Jacob and Baroni, Marco and Bouchacourt, Diane and Lake, Brenden M},
  year = {2020},
  volume = {33},
  pages = {19861--19872},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper/2020/hash/e5a90182cc81e12ab5e72d66e0b46fe3-Abstract.html},
  urldate = {2024-03-18},
  abstract = {Humans easily interpret expressions that describe unfamiliar situations composed from familiar parts ("greet the pink brontosaurus by the ferris wheel"). Modern neural networks, by contrast, struggle to interpret novel compositions. In this paper, we introduce a new benchmark, gSCAN, for evaluating compositional generalization in situated language understanding. Going beyond a related benchmark that focused on syntactic aspects of generalization, gSCAN defines a language grounded in the states of a grid world, facilitating novel evaluations of acquiring linguistically motivated rules. For example, agents must understand how adjectives such as 'small' are interpreted relative to the current world state or how adverbs such as 'cautiously' combine with new verbs. We test a strong multi-modal baseline model and a state-of-the-art compositional method finding that, in most cases, they fail dramatically when generalization requires systematic compositional rules.},
  file = {/Users/shanest/sync/library/Ruis et al/2020/Ruis et al. - 2020 - A Benchmark for Systematic Generalization in Groun.pdf}
}

@article{schlenkerMinimalCompositionalityBird2024,
  title = {Minimal {{Compositionality}} versus {{Bird Implicatures}}:  {{Two Theories}} of {{ABC-D Sequences}} in {{Japanese Tits}}},
  shorttitle = {Minimal {{Compositionality}} versus {{Bird Implicatures}}},
  author = {Schlenker, Philippe and Salis, Ambre and Leroux, Ma{\"e}l and Coye, Camille and Rizzi, Luigi and {Steinert-Threlkeld}, Shane and Chemla, Emmanuel},
  year = {2024},
  journal = {Biological Reviews},
  doi = {10.1111/brv.13068},
  urldate = {2024-02-26},
  abstract = {It was argued in a series of experimental studies that Japanese tits (Parus minor) have an ABC call that has an alert function, a D call that has a recruitment function, and an ABC-D call that is compositionally derived from ABC and D, and has a mobbing function. A key conclusion was that ABC-D differs from the combination of separate utterances of ABC and of D (e.g. as played by distinct but close loudspeakers). While the logic of the argument is arguably sound, no explicit rule has been proposed to derive the meaning of ABC-D from that of its parts. We compare two analyses. One posits a limited instance of semantic compositionality ('Minimal Compositionality'); the other does without compositionality, but with a more sophisticated pragmatics ('Bird Implicatures'). Minimal Compositionality takes the composition of ABC and D to deviate only minimally from what would be found with two independent utterances: ABC means that 'there is something that licenses an alert', D means that 'there is something that licenses recruitment', and ABC-D means that 'there is something that licenses both an alert and recruitment'. By contrast, ABC and D as independent utterances yield something weaker, namely: 'there is something that licenses an alert, and there is something that licenses recruitment', without any 'binding' across the two utterances. The second theory, Bird Implicatures, only requires that ABC-D should be more informative than ABC, and/or than D. It builds on the idea, proposed for several monkey species, that a less informative call competes with a more informative one (`Informativity Principle'): when produced alone, ABC and D trigger an inference that ABC-D is false. We explain how both Minimal Compositionality and Bird Implicatures could have evolved, and we compare the predictions of the two theories. Finally, we extend the discussion to some chimpanzee and meerkat sequences that might raise related theoretical problems.},
  keywords = {animal linguistics,animal semantics,bird calls,compositionality,implicatures,informativity principle,meerkat calls,minimal compositionality,morphology,semantics,syntax},
  annotation = {LingBuzz Published In: To appear in Biological Reviews},
  file = {/Users/shanest/sync/library/Schlenker et al/2024/Schlenker et al. - 2024 - Minimal Compositionality versus Bird Implicatures.pdf;/Users/shanest/Zotero/storage/DW8DDL8T/007422.html}
}

@article{smithLinguisticStructureEvolutionary2013,
  title = {Linguistic Structure Is an Evolutionary Trade-off between Simplicity and Expressivity},
  author = {Smith, Kenny and Tamariz, Monica and Kirby, Simon},
  year = {2013},
  journal = {Proceedings of the 35th Annual Meeting of the Cognitive Science Society},
  pages = {1348--1353},
  abstract = {Language exhibits structure: a species-unique system for expressing complex meanings using complex forms. We present a review of modelling and experimental literature on the evolution of structure which suggests that structure is a cultural adaptation in response to pressure for expressivity (arising during communication) and compressibility (arising during learning), and test this hypothesis using a new Bayesian iterated learning model. We conclude that linguistic structure can and should be explained as a consequence of cultural evolution in response to these two pressures.},
  keywords = {cultural evolution,language,learning,structure},
  file = {/Users/shanest/sync/library/Smith et al/2013/Smith et al. - 2013 - Linguistic structure is an evolutionary trade-off .pdf}
}

@article{steinert-threlkeldEmergenceNontrivialCompositionality2020,
  title = {Toward the {{Emergence}} of {{Nontrivial Compositionality}}},
  author = {{Steinert-Threlkeld}, Shane},
  year = {2020},
  journal = {Philosophy of Science},
  volume = {87},
  number = {5},
  pages = {897--909},
  publisher = {The University of Chicago Press},
  issn = {0031-8248},
  doi = {10.1086/710628},
  urldate = {2022-03-02},
  abstract = {All natural languages exhibit a distinction between content words (nouns, verbs, etc.) and function words (determiners, auxiliaries, tenses, etc.). Yet surprisingly little has been said about the emergence of this universal architectural feature of human language. This article argues that the existence of this distinction requires the presence of nontrivial compositionality and identifies assumptions that have previously been made in the literature that provably guarantee only trivial composition. It then presents a signaling game with variable contexts and shows how the distinction can emerge via reinforcement learning.},
  keywords = {Artificial Intelligence,Cognitive Science,evolution,function,games,language,learning,networks,neural,of,reinforcement,signaling,words},
  file = {/Users/shanest/sync/library/Steinert-Threlkeld/2020/Steinert-Threlkeld - 2020 - Toward the Emergence of Nontrivial Compositionalit.pdf}
}

@article{suzukiAnimalLinguisticsExploring2021,
  title = {Animal Linguistics: {{Exploring}} Referentiality and Compositionality in Bird Calls},
  shorttitle = {Animal Linguistics},
  author = {Suzuki, Toshitaka N.},
  year = {2021},
  journal = {Ecological Research},
  volume = {36},
  number = {2},
  pages = {221--231},
  issn = {1440-1703},
  doi = {10.1111/1440-1703.12200},
  urldate = {2024-03-19},
  abstract = {Establishing the theory of language evolution is an ongoing challenge in science. One profitable approach in this regard is to seek the origins of linguistic capabilities by comparing language with the vocal communication systems of closely related relatives (i.e., the great apes). However, several key capabilities of language appear to be absent in non-human primates, which limits the range of studies, such as direct phylogenetic comparison. A further informative approach lies in identifying convergent features in phylogenetically distant animals and conducting comparative studies. This approach is particularly useful with respect to establishing general rules for the evolution of linguistic capabilities. In this article, I review recent findings on linguistic capabilities in a passerine bird species, the Japanese tit (Parus minor). Field experiments have revealed that Japanese tits produce unique alarm calls when encountering predatory snakes, which serve to enhance the visual attention of call receivers with respect to snake-like objects. Moreover, tits often combine discrete types of meaningful calls into fixed-ordered sequences according to an ordering rule, conveying a compositional message to receivers. These findings indicate that two core capabilities of language, namely, referentiality and compositionality, have independently evolved in the avian lineage. I describe how these linguistic capabilities can be examined under field conditions and discuss how such research may contribute to exploring the origins and evolution of language.},
  copyright = {{\copyright} 2021 The Authors. Ecological Research published by John Wiley \& Sons Australia, Ltd on behalf of The Ecological Society of Japan},
  langid = {english},
  keywords = {bird,communication,compositionality,language evolution,referentiality},
  file = {/Users/shanest/Zotero/storage/ZIE2RINF/Suzuki - 2021 - Animal linguistics Exploring referentiality and c.pdf;/Users/shanest/Zotero/storage/HVEGDXDV/1440-1703.html}
}

@article{szaboCompositionality2022,
  title = {Compositionality},
  author = {Szab{\'o}, Zolt{\'a}n Gendler},
  editor = {Zalta, Edward N},
  year = {2022},
  journal = {The Stanford Encyclopedia of Philosophy},
  volume = {Fall},
  url = {https://plato.stanford.edu/archives/fall2022/entries/compositionality/}
}

@inproceedings{zhouLeasttoMostPromptingEnables2022,
  title = {Least-to-{{Most Prompting Enables Complex Reasoning}} in {{Large Language Models}}},
  booktitle = {The {{Eleventh International Conference}} on {{Learning Representations}}},
  author = {Zhou, Denny and Sch{\"a}rli, Nathanael and Hou, Le and Wei, Jason and Scales, Nathan and Wang, Xuezhi and Schuurmans, Dale and Cui, Claire and Bousquet, Olivier and Le, Quoc V. and Chi, Ed H.},
  year = {2022},
  month = sep,
  url = {https://openreview.net/forum?id=WZH7099tgfM},
  urldate = {2024-03-19},
  abstract = {Chain-of-thought prompting has demonstrated remarkable performance on various natural language reasoning tasks. However, it tends to perform poorly on tasks which requires solving problems harder than the exemplars shown in the prompts. To overcome this challenge of easy-to-hard generalization, we propose a novel prompting strategy, least-to-most prompting. The key idea in this strategy is to break down a complex problem into a series of simpler subproblems and then solve them in sequence. Solving each subproblem is facilitated by the answers to previously solved subproblems. Our experimental results on tasks related to symbolic manipulation, compositional generalization, and math reasoning reveal that least-to-most prompting is capable of generalizing to more difficult problems than those seen in the prompts. A notable finding is that when the GPT-3 code-davinci-002 model is used with least-to-most prompting, it can solve the compositional generalization benchmark SCAN in any split (including length split) with an accuracy of at least 99{\textbackslash}\% using just 14 exemplars, compared to only 16{\textbackslash}\% accuracy with chain-of-thought prompting. This is particularly noteworthy because neural-symbolic models in the literature that specialize in solving SCAN are trained on the entire training set containing over 15,000 examples. We have included prompts for all the tasks in the Appendix.},
  langid = {english},
  file = {/Users/shanest/sync/library/Zhou et al/2022/Zhou et al. - 2022 - Least-to-Most Prompting Enables Complex Reasoning .pdf}
}

@article{zuberbuhlerSyntaxCompositionalityAnimal2019,
  title = {Syntax and Compositionality in Animal Communication},
  author = {Zuberb{\"u}hler, Klaus},
  year = {2019},
  month = nov,
  journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  volume = {375},
  number = {1789},
  pages = {20190062},
  publisher = {Royal Society},
  doi = {10.1098/rstb.2019.0062},
  urldate = {2024-03-19},
  abstract = {Syntax has been found in animal communication but only humans appear to have generative, hierarchically structured syntax. How did syntax evolve? I discuss three theories of evolutionary transition from animal to human syntax: computational capacity, structural flexibility and event perception. The computation hypothesis is supported by artificial grammar experiments consistently showing that only humans can learn linear stimulus sequences with an underlying hierarchical structure, a possible by-product of computationally powerful large brains. The structural flexibility hypothesis is supported by evidence of meaning-bearing combinatorial and permutational signal sequences in animals, with sometimes compositional features, but no evidence for generativity or hierarchical structure. Again, animals may be constrained by computational limits in short-term memory but possibly also by limits in articulatory control and social cognition. The event categorization hypothesis, finally, posits that humans are cognitively predisposed to analyse natural events by assigning agency and assessing how agents impact on patients, a propensity that is reflected by the basic syntactic units in all languages. Whether animals perceive natural events in the same way is largely unknown, although event perception may provide the cognitive grounding for syntax evolution. This article is part of the theme issue `What can animal communication teach us about human language?'},
  keywords = {grammar,language evolution,meaning,permutation,primate communication,semantics},
  file = {/Users/shanest/Zotero/storage/32WCV59W/Zuberbhler - 2019 - Syntax and compositionality in animal communicatio.pdf}
}
