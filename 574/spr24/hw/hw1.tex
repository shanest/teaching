\documentclass[11pt]{article}
\usepackage{hyperref}
\usepackage[margin=0.75in]{geometry}

\begin{document}

\title{LING 574 HW1}
\date{\vspace{-0.2in}Due 11PM on Apr \textbf{6}, 2024}
\author{}
\maketitle


\paragraph{Nota bene:} This homework is due on April 6, as opposed to April 4.  This if for two reasons: (i) there is scheduled downtime of a large number of UW IT services, including in the room where the patas cluster lives, on April 4 and (ii) the course directory has not yet been available on patas (but will be soon, pending some sysadmin details).  This means that you cannot yet do Section 2 of this assignment, but can still start on Sections 1 and 3.  There will be no late submissions for this assignment given this delayed deadline (alternatively, you can conceptualize it as a late submission with no penalty).

\section{Setting Up Environment on Patas [10 pts]}

Install Anaconda. This is a necessary component for running the code for most assignments in this course.  We have setup a conda environment for the assignment, but you will need to install anaconda in order to use that environment.  These are ``free points''.  From your home directory, please execute the following steps:
\begin{enumerate}
	\item \texttt{wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86\_64.sh}
	\item \texttt{sh Miniconda3-latest-Linux-x86\_64.sh}
\end{enumerate}
The first two lines of \texttt{run\_hw1.sh} in the course folder mentioned below show how to now activate the environment that we have supplied with all of the necessary libraries.


\section{Implementing a Vocabulary Class [30 pts]}

A \texttt{Vocabulary} object provides the basic interface between raw text (strings) and integer IDs, which the models we will build require.  In \texttt{/dropbox/23-24/574/hw1} you will find:
\begin{itemize}
  \item vocabulary.py: main file to edit
  \item main.py: running script, also to edit
  \item run\_hw1.sh: main shell script, add your commands here
  \item toy-vocab.txt: output of running 
  
    \texttt{python main.py --text\_file /dropbox/23-24/574/data/toy-reviews.txt --output\_file toy-vocab.txt}
\end{itemize}
All places for you to edit are marked by `\# TODO:' comments.

\vspace{2em}
\noindent {\bf Q1: populate index\_to\_token} Near the top of \texttt{Vocabulary.\_\_init\_\_}, you must use a Counter object to populate the list \texttt{self.index\_to\_token}.  Some notes:
\begin{itemize}
  \item You should iterate in the order that the Counter iterates.
  \item You should not include any tokens that occure below \texttt{min\_freq} times.
  \item You should only add the first \texttt{max\_size} tokens
\end{itemize}

\vspace{2em}
\noindent {\bf Q2: populate token\_to\_index}  At the end of \texttt{Vocabulary.\_\_init\_\_}, you need to populate a token-to-string dictionary.  Keys are tokens, and values are their integer indices.

\vspace{2em}
\noindent {\bf Q3: implement two methods}  Two short methods require implementation: \texttt{tokens\_to\_indices} and \texttt{indices\_to\_tokens}.

\vspace{2em}
\noindent {\bf Q4: implement main.py}  main.py is a simple script: it takes in a text input file, and an output file name.  You should define a Vocabulary object based on that text file, and then save/write it to the output file.  The Vocabulary class has methods to help with both of those steps.

\vspace{2em}
\noindent {\bf Q5: build two vocabularies}  Finally, execute the following two commands (and add them to \texttt{run\_hw1.sh}):
\begin{itemize}
  \item \texttt{python main.py --text\_file /dropbox/23-24/574/data/sst/train-reviews.txt --output\_file train\_vocab\_base.txt}
  \item \texttt{python main.py --text\_file /dropbox/23-24/574/data/sst/train-reviews.txt --output\_file train\_vocab\_freq5.txt --min\_freq 5}
\end{itemize}
These will produce two output files that should also be included in your tar.gz (see below).


\section{Data Statement for Stanford Sentiment Treebank [35 pts]}

For natural language processing applications, data plays a crucial role, since it largely shapes the resulting models and systems that are used in deployment.  Emily Bender and Batya Friedman have recently been developing and advocating for the practice of ``data statements'': explicit documentation of the nature and origins of datasets used in NLP.  For more information, please consult their paper:\footnote{Note that an updated version of the schema has since been released and is available here: \url{http://techpolicylab.uw.edu/data-statements/}.}
\begin{itemize}
  \item ``\href{https://www.aclweb.org/anthology/Q18-1041/}{Data Statements for Natural Language Processing: Toward Mitigating System Bias and Enabling Better Science}'', \emph{Transactions of the ACL}
\end{itemize}
For the first half of this course, we will make heavy use of the Stanford Sentiment Treebank.  Before beginning to use this data in modeling, you will attempt to write a data statement for this dataset to the best of your ability given the documentation available.  Afterwards, there will be space to reflect on what was missing. Please see:
\begin{itemize}
  \item The paper presenting the data (\S3 in particular): \url{https://www.aclweb.org/anthology/D13-1170/}
  \item The website for the dataset: \url{https://nlp.stanford.edu/sentiment/index.html}
\end{itemize}
Please answer, in a couple of sentences, the following questions. [Each question is 5 points.]

\vspace{2em}
\noindent {\bf Q1: Curation Rationale} Which texts were included  and  what  were  the  goals  in  selecting texts?

\vspace{2em}
\noindent {\bf Q2: Language Variety} From what language variety are the texts?  You may provide a prose description and/or a \href{https://tools.ietf.org/rfc/bcp/bcp47.txt}{BCP47} code.

\vspace{2em}
\noindent {\bf Q3: Speaker Demographics} Who were the producers of the texts?  Information may include (if available): age, gender, native language, socioeconomic status, number of speakers.

\vspace{2em}
\noindent {\bf Q4: Annotator Demographics} Who were the annotators of the data?  Information may include (if available): age, gender, native language, socioeconomic status, number of annotators.

\vspace{2em}
\noindent {\bf Q5: Speech Situation} What were the conditions of text production?  Details may include whether it was written or spoken, whether it was spontaneous or not, and who the intended audience was.

\vspace{2em}
\noindent {\bf Q6: Text Characteristics} What are the genre and topic of the texts?

\vspace{2em}
\noindent {\bf Q7: Reflections}  Which of these questions were hard to answer with the information provided about the dataset?  Why might it be helpful to have that information more explicitly documented?

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Submission Instructions}

In your submission, include the following:
\begin{itemize}
  \item readme.(txt$\mid$pdf) that includes your answers to \S3 Q1-Q7. 
  \item \texttt{hw1.tar.gz} containing:
  \begin{itemize}
    \item vocabulary.py
    \item main.py
    \item run\_hw1.sh
    \item train\_vocab\_base.txt
    \item train\_vocab\_freq5.txt
  \end{itemize}
\end{itemize}





\end{document}



